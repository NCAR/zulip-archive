<html>
<head><meta charset="utf-8"><title>memory usage · dask · Zulip Chat Archive</title></head>
<h2>Stream: <a href="../../..//stream/27-dask/index.html">dask</a></h2>
<h3>Topic: <a href="../../..//stream/27-dask/topic/memory.20usage.html">memory usage</a></h3>

<hr>


<head><link href="../../../style.css" rel="stylesheet"></head>

<a name="23095"></a>
<h4><a href="https://zulip2.cloud.ucar.edu#narrow/stream/27-dask/topic/memory%20usage/near/23095" class="zl"><img src="../../../assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Ufuk Turuncoglu <a href="../../..//stream/27-dask/topic/memory.20usage.html#23095">(Dec 15 2020 at 21:42)</a>:</h4>
<p>I am trying to write a script to process very-high resolution dataset (GHRSST) and when i instrument my code with memory_profiler i am seeing following statements adds extra memory consumption and i just wonder that is it possible to rewrite them to reduce the memory usage.</p>
<p><strong>Statement 1:</strong></p>
<div class="codehilite"><pre><span></span>   97                                             # REPLACED: corner_pair_uniq = dd.from_dask_array(corner_pair).drop_duplicates().to_dask_array(lengths=True)
    98                                             # following reduces memory by %17
    99  258.629 MiB    0.680 MiB           1       corner_pair_uniq = dd.from_dask_array(corner_pair).drop_duplicates().values
   100 1005.586 MiB  746.957 MiB           1       corner_pair_uniq.compute_chunk_sizes()
</pre></div>


<p>In this case i reduced the memory consumption by changing the calculation of corner_pair_uniq but there might be another way to reduce more.</p>
<p><strong>Statement 2:</strong></p>
<div class="codehilite"><pre><span></span>   113 1005.586 MiB    0.000 MiB           5       corners = dd.concat([dd.from_dask_array(c) for c in [corner_lon.T.reshape((-1,)).T, corner_lat.T.reshape((-1,)).T]], axis=1)
   114 1005.586 MiB    0.000 MiB           1       corners.columns = [&#39;lon&#39;, &#39;lat&#39;]
   115 1789.883 MiB  784.297 MiB           1       elem_conn = corners.compute().groupby([&#39;lon&#39;,&#39;lat&#39;], sort=False).ngroup()+1
   116 1692.887 MiB  -96.996 MiB           1       elem_conn = da.from_array(elem_conn.to_numpy())
</pre></div>


<p>Calculation of elem_conn introduces another jump in the memory.  Any suggestion?</p>



<hr><p>Last updated: May 16 2025 at 17:14 UTC</p>
</html>