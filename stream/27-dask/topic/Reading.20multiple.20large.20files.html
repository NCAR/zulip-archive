<html>
<head><meta charset="utf-8"><title>Reading multiple large files · dask · Zulip Chat Archive</title></head>
<h2>Stream: <a href="http://127.0.0.1:4000/html/stream/27-dask/index.html">dask</a></h2>
<h3>Topic: <a href="http://127.0.0.1:4000/html/stream/27-dask/topic/Reading.20multiple.20large.20files.html">Reading multiple large files</a></h3>

<hr>


<head><link href="http://127.0.0.1:4000/style.css" rel="stylesheet"></head>

<a name="93321"></a>
<h4><a href="https://zulip2.cloud.ucar.edu#narrow/stream/27-dask/topic/Reading%20multiple%20large%20files/near/93321" class="zl"><img src="../../../assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Mira Berdahl <a href="http://127.0.0.1:4000/html/stream/27-dask/topic/Reading.20multiple.20large.20files.html#93321">(Dec 08 2023 at 22:26)</a>:</h4>
<p>Hi,<br>
I'm trying to read in some large, high resolution (0.1deg), ocean (POP) output.<br>
In the past, I've used the following without issue (with 1deg ocean), but the 0.1deg files are bigger and so it stalls out and crashes. Can anyone help me optimize this so it runs more efficiently? I'm running on JupyterHub.</p>
<p>import numpy as np<br>
import xarray as xr<br>
#from matplotlib import pyplot as plt<br>
import <a href="http://cartopy.crs">cartopy.crs</a> as ccrs<br>
import nc_time_axis as nc_time_axis<br>
import cmocean<br>
import zarr<br>
import <a href="http://numpy.ma">numpy.ma</a> as ma<br>
import pop_tools</p>
<p>import matplotlib.pyplot as plt<br>
from distributed import Client<br>
from ncar_jobqueue import NCARCluster</p>
<h1>on cheyenne</h1>
<p>cluster = NCARCluster(project = 'ULNL0002', memory="125GB", walltime='1:00:00', cores=4, processes=4, resource_spec='select=1:ncpus=4:mem=125GB')<br>
cluster.scale(32)<br>
#cluster.adapt(minimum_jobs=1, maximum_jobs=5)<br>
client = Client(cluster)<br>
cluster</p>
<p>#############################################################################################################<br>
from glob import glob</p>
<p>################### READ 0.1degree data ######################################################<br>
ddir = '/glade/campaign/collections/cmip/CMIP6/CESM-HR/BHIST/HR/b.e13.BHISTC5.ne120_t12.cesm-ihesp-sehires38-1850-2005.001/ocn/proc/tseries/month_1/'<br>
dfiles = sorted(glob(ddir + '<em>.TEMP.</em>.nc'))  # use sorted to make sure the files are in order for concatenation</p>
<h4>a tried and true method from Anderson.</h4>
<p>mfds = xr.open_mfdataset(dfiles, combine='by_coords', parallel=True , chunks={'time': 6}, data_vars=['TEMP', 'time_bound'], decode_times=False) <br>
mfds = xr.decode_cf(fixmonth(mfds)) <br>
mfds<br>
############################################################################################</p>



<a name="93367"></a>
<h4><a href="https://zulip2.cloud.ucar.edu#narrow/stream/27-dask/topic/Reading%20multiple%20large%20files/near/93367" class="zl"><img src="../../../assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> David John Gagne <a href="http://127.0.0.1:4000/html/stream/27-dask/topic/Reading.20multiple.20large.20files.html#93367">(Dec 11 2023 at 16:26)</a>:</h4>
<p>If you are loading a larger amount of data, I would increase the number of CPUs and memory you are requesting so that you can take advantage of parallelism and larger nodes. If you are running on casper, you can ask for up to 384 GB and 36 CPUs per node. </p>
<p>If that doesn't work, then I would suggest performing analysis in a way that does not require opening all the files at once. You may have to write a more manual loop instead of using open_mfdataset.</p>



<hr><p>Last updated: May 16 2025 at 17:14 UTC</p>
</html>