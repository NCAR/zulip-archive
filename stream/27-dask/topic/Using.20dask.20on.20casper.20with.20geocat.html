<html>
<head><meta charset="utf-8"><title>Using dask on casper with geocat · dask · Zulip Chat Archive</title></head>
<div class='page-content'><h2>Stream: <a href="https://ncar.github.io/zulip-archive/stream/27-dask/index.html">dask</a></h2>
<h3>Topic: <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html">Using dask on casper with geocat</a></h3>

<hr>

<base href="https://zulip2.cloud.ucar.edu/">

<head><link href="https://ncar.github.io/zulip-archive/style.css" rel="stylesheet"></head>

<a name="44239"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44239" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Brian Medeiros <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44239">(Oct 04 2021 at 16:49)</a>:</h4>
<p>I'd like to use geocat's <code>interp_hybrid_to_pressure</code> function on a large series of simulations, but it seems very slow. I noticed that its output is a dask object, so I figured this was a good opportunity to try to learn how to use dask better on casper. Unfortunately, my attempt has totally failed. Using a single dataarray, the function no longer even completes the interpolation step (takes about a minute without setting up the dask cluster). I tried to follow the example of setting up dask from <a href="https://ncar.github.io/esds/posts/2021/casper_pbs_dask/">https://ncar.github.io/esds/posts/2021/casper_pbs_dask/</a> and I upped the resources to be 6 cores, 128GB memory, and 12 processes.  Here's how I tried to set up the cluster:</p>
<div class="codehilite"><pre><span></span><code># Import dask
import dask

# Use dask jobqueue
from dask_jobqueue import PBSCluster

# Import a client
from dask.distributed import Client

# Setup your PBSCluster
cluster = PBSCluster(
    cores=6, # The number of cores you want
    memory=&#39;128GB&#39;, # Amount of memory
    processes=12, # How many processes
    queue=&#39;casper&#39;, # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)
    local_directory=&#39;$TMPDIR&#39;, # Use your local directory
    resource_spec=&#39;select=1:ncpus=6:mem=128GB&#39;, # Specify resources
    project=&#39;P03010039&#39;, # Input your project ID here
    walltime=&#39;02:00:00&#39;, # Amount of wall time
    interface=&#39;ib0&#39;, # Interface to use
)


client = Client(cluster)
# Setup your PBSCluster - make sure that it uses the casper queue
cluster = PBSCluster(queue=&#39;casper&#39;)
</code></pre></div>
<p>My script is here: <code>/glade/u/home/brianpm/example_dask_geocat.py</code>.  </p>
<p>Anyone have an idea where I've gone off the rails here?</p>



<a name="44244"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44244" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Isla Simpson <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44244">(Oct 04 2021 at 17:10)</a>:</h4>
<p>Hi Brian,  I'm not sure I can be helpful in seeing what's going on but I have  been using the interp_hybrid_to_pressure with some success.  I was trying to do things quickly and rather than using geocat I just pasted the function into my notebooks.  An examples is here...<br>
/glade/u/home/islas/scripts/setupSNAP/postprocess/6hrZ/postprocessdata_6hrZ_3D.ipynb.  It does seem slow to be compared to NCL, but it does actually complete and is not terribly slow.  Glad to hear if there are more efficient ways or if there's something that I have set here that makes a difference.</p>



<a name="44245"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44245" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Matt Long <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44245">(Oct 04 2021 at 17:13)</a>:</h4>
<p>cc <span class="user-group-mention" data-user-group-id="1">@geocat</span></p>



<a name="44250"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44250" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Brian Medeiros <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44250">(Oct 04 2021 at 17:21)</a>:</h4>
<p>Thanks, <span class="user-mention" data-user-id="45">@Isla Simpson</span> . Yeah, the function itself works okay. My backup plan is to use the version of the function that I had passed on to the geocat folks and wrap it using Numba, which makes it much faster by compiling the function. I'm hoping that I can someday figure out how to get dask to speed up code (so far my experience seems to go in the opposite direction).</p>



<a name="44251"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44251" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Orhan Eroglu <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44251">(Oct 04 2021 at 17:22)</a>:</h4>
<p><span class="user-mention" data-user-id="61">@Brian Medeiros</span> <span class="user-mention" data-user-id="45">@Isla Simpson</span> these are great cases for us to test <code>interp_hybrid_to_pressure</code> with real data sets. I will try to work on both of your cases with <span class="user-mention" data-user-id="114">@Anissa Zacharias</span>, who did a lot of Dask investigations recently, once both of us have some time. That way we can see if there are bottlenecks with our function flow and/or Dask use. Will keep you all updated.</p>



<a name="44268"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44268" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Katie Dagon <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44268">(Oct 04 2021 at 20:49)</a>:</h4>
<p><span class="user-mention" data-user-id="61">@Brian Medeiros</span> Is it possible you need to add a <code>cluster.scale</code> command before the <code>Client(cluster)</code> command? This is something I do following the examples I've seen. I think it relates to asking for a certain number of dask workers, but I'm not always sure how to set the number. I also noticed you have a second <code>cluster = PBSCluster()</code> command at the bottom of your code example above - might this be resetting the previous PBSCluster command?<br>
When using dask in a Jupyter notebook, I have also found the dask dashboard  very helpful to verify that the workers are ready, and for monitoring progress.</p>



<a name="44558"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44558" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Orhan Eroglu <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44558">(Oct 07 2021 at 16:21)</a>:</h4>
<p>Hi <span class="user-mention" data-user-id="61">@Brian Medeiros</span>  and maybe <span class="user-mention" data-user-id="45">@Isla Simpson</span> , </p>
<p><span class="user-mention" data-user-id="114">@Anissa Zacharias</span> and I worked on Brian's code and were able to get it run successfully in about 25 seconds on Casper with Dask. I think it is a good wall time for such a big data set (let us know your thought though), but it could be made a bit better if we play with the chunk sizes etc. more.  </p>
<p>I'd like to share a few findings of ours with your code below, and I am happy to have a video chat with you and others in this topic to go through the details and get your end up and running (e.g. looking into your personal Dask configs on glade etc.):</p>
<ul>
<li>
<p>First of all, you can find two notebooks with your code corrected (<code>Anissa.ipynb</code> and <code>Orhan.ipynb</code>) at <code>/glade/u/home/oero/src/brian_issue_interp_hybrid_to_sigma</code>. What we did as corrections in those notebooks are itemized below:</p>
</li>
<li>
<p>We agree with <span class="user-mention" data-user-id="47">@Katie Dagon</span> 's suggestions: In your code, you already "Setup your PBSCluster"  in the larger block with several resource parameters determined by you, then your last line of code (<code>cluster = PBSCluster(queue='casper')</code>) discards whatever cluster you already set up in that larger block. That final line needs to be removed. In addition, we recommend to use <code>cluster.scale</code>, too.</p>
</li>
<li>
<p>You may also consider using <code>NCARCluster</code> instead of <code>PBSCluster</code> to set up your cluster as we did in the notebook. It is just more convenience rather than anything as a technical difference under the hood. To use NCARCluster, you will need to setup a config file for yourself as described <a href="https://github.com/NCAR/ncar-jobqueue#configuration">here</a>.  </p>
</li>
<li>
<p>WHatever you are using to setup your cluster/client, you alwayw need to check first if your cluster and client are being setup successfully. You can leverage calling <code>client</code> after setting up your cluster/client, and it will show you the client info, Dask dashboard link, and number of workers etc. to show you that you have your cluster/client up and ready for Dask calculations. I am happy to wlalk you through this in a video chat. </p>
</li>
<li>
<p>Dask setup is sometimes ambigous, and if you don't check your cluster/client as in the previous bullet point, you may think your function is stuck, but the reality might be just that you don't have your client up, or your workers have been allocated the resources you requested etc. For example, the latetr is the case I have been currently facing on my end: For some reason, I cannot get the resources allocated that I request for my NCARCluster setup. I am using all the configs the same with <span class="user-mention" data-user-id="114">@Anissa Zacharias</span> 's  ones, but I haven't been able to get resources for my notebook on Casper yet. I may need further support from someone else here.</p>
</li>
<li>
<p>Once you have your cluster/client up, you should be good to successfully run <code>interp_hybrid_to_sigma</code> as we are able to do so on <span class="user-mention" data-user-id="114">@Anissa Zacharias</span> 's end.</p>
</li>
<li>
<p>There are some other details we played with in your code: </p>
<ul>
<li>For the large data arrays (such as <code>ocld</code> and maybe <code>e_ps</code> in yours), using <code>.persist()</code> as we did in your code could make the calculations  efficient in distributed environments such as Casper.    </li>
<li>In your code, you were chunking the output (<code>ocld_plev</code>) of the function. It will only benefit you if you will use that output as an input to another function. Instead, chunking your large input arrays before feeding them into the function you run (i.e.<code>interp_hybrid_to_pressure</code> in this case) will benefit you with Dask's distributed performance. Please see below:<br>
<code>ocld_chunked = ocld.chunk({'time':10, 'lat':192, 'lon':288})</code><div class="codehilite"><pre><span></span><code>`ocld_plev = gc.interp_hybrid_to_pressure(ocld_chunked, e_ps, hyam, hybm, p0=pref, new_levels=np.arange(100000., 5000., -2500), method=&#39;linear&#39;) #.compute()`
</code></pre></div>

</li>
</ul>
</li>
</ul>
<p>We hope this helps. Let us know if you need to further discuss anything and/or work with us to set up your end.</p>



<a name="44580"></a>
<h4><a href="https://zulip2.cloud.ucar.edu/#narrow/stream/27-dask/topic/Using%20dask%20on%20casper%20with%20geocat/near/44580" class="zl"><img src="https://ncar.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Brian Medeiros <a href="https://ncar.github.io/zulip-archive/stream/27-dask/topic/Using.20dask.20on.20casper.20with.20geocat.html#44580">(Oct 07 2021 at 17:12)</a>:</h4>
<p>Thanks <span class="user-mention" data-user-id="18">@Orhan Eroglu</span> and <span class="user-mention" data-user-id="114">@Anissa Zacharias</span> !! There's a lot here; I'll go through it in detail as soon as I can.  I'll be in touch if I have follow-up questions.</p>



<hr><p>Last updated: Jan 30 2022 at 12:01 UTC</p>
</html></div>