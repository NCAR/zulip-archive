[
    {
        "content": "<p>This topic was moved by <span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> to <a class=\"stream-topic\" data-stream-id=\"27\" href=\"/#narrow/stream/27-dask/topic/KilledWorker.20Error\">#dask &gt; KilledWorker Error</a></p>",
        "id": 52309,
        "sender_full_name": "Notification Bot",
        "timestamp": 1647523438
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"10\">@Michael Levy</span> , following up from what we were working on today. I used the DASK set up you suggested as well as split up the notebook and am trying to just process the small experiment with 5 ens members, but I'm getting the killed worker error when trying to write my 500MB file. If you're willing to look at this again, I'd appreciate your input. <br>\nNotebook: /glade/p/cgd/ppc/duvivier/cesm2_arctic_cyclones/rufmod_analysis/version_3/wind_vertical/rufmod_wind_vertical_processing.ipynb</p>\n<p>Error: <code>KilledWorker: (\"('_vertical_remap-12d6f63cd38f9fd5cbb1291da0026378', 0, 0, 0, 0, 0)\", &lt;WorkerState 'tcp://10.12.206.37:33228', name: PBSCluster-0-5, status: closed, memory: 0, processing: 1&gt;)</code></p>",
        "id": 52793,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1648528146
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span> I'm happy to look at this again -- if you want to chat about it, my only meeting today is at 2:00 so I'm available any other tie</p>",
        "id": 52800,
        "sender_full_name": "Michael Levy",
        "timestamp": 1648563685
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"10\">@Michael Levy</span> and I worked on this today. Basically it was a question of making sure the chunk-size and number of chunks was reasonable and then asking for more memory, processes, and cores in the dask request. With those changes everything works now.</p>",
        "id": 52905,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1648603145
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask.3A.20KilledWorker.20Error/near/52905\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"10\">Michael Levy</span> and I worked on this today. Basically it was a question of making sure the chunk-size and number of chunks was reasonable and then asking for more memory, processes, and cores in the dask request. With those changes everything works now.</p>\n</blockquote>\n<p>This would be a cool short blog post! Just a before/after comparison.</p>",
        "id": 53007,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1648744937
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"25\">Deepak Cherian</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask.3A.20KilledWorker.20Error/near/53007\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask.3A.20KilledWorker.20Error/near/52905\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"10\">Michael Levy</span> and I worked on this today. Basically it was a question of making sure the chunk-size and number of chunks was reasonable and then asking for more memory, processes, and cores in the dask request. With those changes everything works now.</p>\n</blockquote>\n<p>This would be a cool short blog post! Just a before/after comparison.</p>\n</blockquote>\n<p>I agree that it would be a great blog post to have - I could give it a shot, but I worry that my justification for the <code>chunk</code> choices and some arguments we picked for <code>PBSCluster()</code> will be \"well, we tried it and it worked so that was cool.\" I think the big thing was running the cluster with</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">PBSCluster</span><span class=\"p\">(</span>\n    <span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"mi\">36</span><span class=\"p\">,</span>\n    <span class=\"n\">memory</span><span class=\"o\">=</span><span class=\"s1\">'300 GB'</span><span class=\"p\">,</span>\n    <span class=\"n\">processes</span><span class=\"o\">=</span><span class=\"mi\">9</span><span class=\"p\">,</span>\n    <span class=\"n\">resource_spec</span><span class=\"o\">=</span><span class=\"s1\">'select=1:ncpus=36:mem=300GB'</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n<p>I would love to have a permanent link to that reference instead of needing to remember which of my notebooks actually use <code>dask-jobqueue</code> and are configured correctly.</p>\n<p>Another concern about writing the blog post is that it is still unclear to me if we should explicitly recommend other arguments to avoid potential conflicts with <code>~/.config/dask/jobqueue.yaml</code> (or if we should be recommending any settings for that file instead / in addition). <span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span> might have been explicitly setting <code>queue</code>, <code>walltime</code>, and / or <code>interface</code>, and obviously we should mention <code>project</code></p>",
        "id": 53008,
        "sender_full_name": "Michael Levy",
        "timestamp": 1648745448
    },
    {
        "content": "<blockquote>\n<p>my justification for the chunk choices and some arguments we picked for PBSCluster() will be \"well, we tried it and it worked so that was cool.\" </p>\n</blockquote>\n<p>I think this is totally fine.</p>\n<blockquote>\n<p>. Another concern about writing the blog post is that it is still unclear to me if we should explicitly recommend other arguments to avoid potential conflicts </p>\n</blockquote>\n<p>I think we can view the blogpost as trying  to reason about when such modifications might be useful.</p>",
        "id": 53011,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1648745637
    },
    {
        "content": "<p>It was also things like setting explicitly the chunks argument. E.g. <br>\n| ds4_1=xr.open_mfdataset(my_files,combine='by_coords',chunks={'time':129}, parallel=True, compat='override', coords='minimal')</p>\n<p>I do think some guidance on when is the \"best\" time to load data would be good. Like how would one choose when it's appropriate to do so? A lot of the calculations are the \"lazy\" kind in python, and this was new to me after using NCL and such. So some guidance on when and how to actually load the data would be great.</p>",
        "id": 53014,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1648749312
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask.3A.20KilledWorker.20Error/near/53014\">said</a>:</p>\n<blockquote>\n<p>I do think some guidance on when is the \"best\" time to load data would be good. Like how would one choose when it's appropriate to do so? A lot of the calculations are the \"lazy\" kind in python, and this was new to me after using NCL and such. So some guidance on when and how to actually load the data would be great.</p>\n</blockquote>\n<p>Yeah hearing from others about this would be great. I tend to wait until I need to start plotting something, and then use <code>.persist()</code> on the final data array (after all the computations) so that it's easier to re-run the plot code over and over as I'm tweaking the visualization.</p>",
        "id": 53015,
        "sender_full_name": "Katie Dagon",
        "timestamp": 1648749496
    },
    {
        "content": "<p>I guess I don't really know why one would choose <code>.persist()</code> vs. <code>.load()</code> in different circumstances. I often try one or the other if it isn't working. For this particular notebook, it took long enough to do the calculations that I saved the data I wanted to plot as a netcdf file and then in a separate notebook do the plotting so as I tweak the visualization I don't have to worry about re-running the data manipulation if I time out or something. I don't always do that, but in this case I did.</p>",
        "id": 53017,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1648760066
    },
    {
        "content": "<p><code>.persist()</code> keeps the memory distributed; <code>.load()</code> loads everything locally.</p>",
        "id": 53021,
        "sender_full_name": "Matt Long",
        "timestamp": 1648760738
    },
    {
        "content": "<p>I found these documentation pages to be helpful in understanding the differences between <code>.persist()</code>, <code>.compute()</code>, and <code>.load()</code>:<br>\n<a href=\"https://distributed.dask.org/en/stable/memory.html\">https://distributed.dask.org/en/stable/memory.html</a><br>\n<a href=\"https://distributed.dask.org/en/latest/manage-computation.html\">https://distributed.dask.org/en/latest/manage-computation.html</a><br>\n<a href=\"https://xarray.pydata.org/en/stable/user-guide/dask.html#using-dask-with-xarray\">https://xarray.pydata.org/en/stable/user-guide/dask.html#using-dask-with-xarray</a></p>",
        "id": 53024,
        "sender_full_name": "Katie Dagon",
        "timestamp": 1648761316
    }
]