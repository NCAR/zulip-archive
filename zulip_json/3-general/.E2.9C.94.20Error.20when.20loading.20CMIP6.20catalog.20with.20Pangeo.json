[
    {
        "content": "<p>I'm trying to follow an online example on my own on Casper using the NPL 2024a environment. This it he first time trying to do this so I'm not familiar with the requirements and packages. I follow more or less exactly this example:</p>\n<p><a href=\"https://nordicesmhub.github.io/forces-2021/learning/example-notebooks/xesmf_regridding.html\">https://nordicesmhub.github.io/forces-2021/learning/example-notebooks/xesmf_regridding.html</a></p>\n<p>When calling this line:</p>\n<div class=\"codehilite\"><pre><span></span><code>dset_dict = cat.to_dataset_dict(zarr_kwargs={&#39;use_cftime&#39;:True})\n</code></pre></div>\n<p>I receive the following error message (abbreviated for length): </p>\n<div class=\"codehilite\"><pre><span></span><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/fsspec/registry.py:236, in get_filesystem_class(protocol)\n    235 try:\n--&gt; 236     register_implementation(protocol, _import_class(bit[&quot;class&quot;]))\n    237 except ImportError as e:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/fsspec/registry.py:271, in _import_class(cls, minv)\n    270 s3 = mod == &quot;s3fs&quot;\n--&gt; 271 mod = importlib.import_module(mod)\n    272 if s3 and mod.__version__.split(&quot;.&quot;) &lt; [&quot;0&quot;, &quot;5&quot;]:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/importlib/__init__.py:126, in import_module(name, package)\n    125         level += 1\n--&gt; 126 return _bootstrap._gcd_import(name[level:], package, level)\n\nFile &lt;frozen importlib._bootstrap&gt;:1204, in _gcd_import(name, package, level)\n\nFile &lt;frozen importlib._bootstrap&gt;:1176, in _find_and_load(name, import_)\n\nFile &lt;frozen importlib._bootstrap&gt;:1140, in _find_and_load_unlocked(name, import_)\n\nModuleNotFoundError: No module named &#39;gcsfs&#39;\n\nThe above exception was the direct cause of the following exception:\n\nImportError                               Traceback (most recent call last)\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/intake_esm/source.py:244, in ESMDataSource._open_dataset(self)\n    223 datasets = [\n    224     _open_dataset(\n    225         record[self.path_column_name],\n   (...)\n    241     for _, record in self.df.iterrows()\n    242 ]\n--&gt; 244 datasets = dask.compute(*datasets)\n    245 if len(datasets) == 1:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/dask/base.py:665, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    664 with shorten_traceback():\n--&gt; 665     results = schedule(dsk, keys, **kwargs)\n    667 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/intake_esm/source.py:77, in _open_dataset(urlpath, varname, xarray_open_kwargs, preprocess, requested_variables, additional_attrs, expand_dims, data_format, storage_options)\n     76 else:\n---&gt; 77     ds = xr.open_dataset(url, **xarray_open_kwargs)\n     78     if preprocess is not None:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/api.py:572, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    571 overwrite_encoded_chunks = kwargs.pop(&quot;overwrite_encoded_chunks&quot;, None)\n--&gt; 572 backend_ds = backend.open_dataset(\n    573     filename_or_obj,\n    574     drop_variables=drop_variables,\n    575     **decoders,\n    576     **kwargs,\n    577 )\n    578 ds = _dataset_from_backend_dataset(\n    579     backend_ds,\n    580     filename_or_obj,\n   (...)\n    590     **kwargs,\n    591 )\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/zarr.py:1011, in ZarrBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\n   1010 filename_or_obj = _normalize_path(filename_or_obj)\n-&gt; 1011 store = ZarrStore.open_group(\n   1012     filename_or_obj,\n   1013     group=group,\n   1014     mode=mode,\n   1015     synchronizer=synchronizer,\n   1016     consolidated=consolidated,\n   1017     consolidate_on_close=False,\n   1018     chunk_store=chunk_store,\n   1019     storage_options=storage_options,\n   1020     stacklevel=stacklevel + 1,\n   1021     zarr_version=zarr_version,\n   1022 )\n   1024 store_entrypoint = StoreBackendEntrypoint()\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/backends/zarr.py:441, in ZarrStore.open_group(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\n    440 try:\n--&gt; 441     zarr_group = zarr.open_consolidated(store, **open_kwargs)\n    442 except KeyError:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/zarr/convenience.py:1334, in open_consolidated(store, metadata_key, mode, **kwargs)\n   1333 zarr_version = kwargs.get(&quot;zarr_version&quot;)\n-&gt; 1334 store = normalize_store_arg(\n   1335     store, storage_options=kwargs.get(&quot;storage_options&quot;), mode=mode, zarr_version=zarr_version\n   1336 )\n   1337 if mode not in {&quot;r&quot;, &quot;r+&quot;}:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/zarr/storage.py:197, in normalize_store_arg(store, storage_options, mode, zarr_version)\n    196     raise ValueError(&quot;zarr_version must be either 2 or 3&quot;)\n--&gt; 197 return normalize_store(store, storage_options, mode)\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/zarr/storage.py:167, in _normalize_store_arg_v2(store, storage_options, mode)\n    166 if &quot;://&quot; in store or &quot;::&quot; in store:\n--&gt; 167     return FSStore(store, mode=mode, **(storage_options or {}))\n    168 elif storage_options:\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/zarr/storage.py:1377, in FSStore.__init__(self, url, normalize_keys, key_separator, mode, exceptions, dimension_separator, fs, check, create, missing_exceptions, **storage_options)\n   1376     storage_options[&quot;auto_mkdir&quot;] = True\n-&gt; 1377 self.map = fsspec.get_mapper(url, **{**mapper_options, **storage_options})\n   1378 self.fs = self.map.fs  # for direct operations\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/fsspec/mapping.py:245, in get_mapper(url, check, create, missing_exceptions, alternate_root, **kwargs)\n    244 # Removing protocol here - could defer to each open() on the backend\n--&gt; 245 fs, urlpath = url_to_fs(url, **kwargs)\n    246 root = alternate_root if alternate_root is not None else urlpath\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/fsspec/core.py:383, in url_to_fs(url, **kwargs)\n    382 kwargs = {k: v for k, v in kwargs.items() if k not in known_kwargs}\n--&gt; 383 chain = _un_chain(url, kwargs)\n    384 inkwargs = {}\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/fsspec/core.py:332, in _un_chain(path, kwargs)\n    331 protocol = kwargs.pop(&quot;protocol&quot;, None) or split_protocol(bit)[0] or &quot;file&quot;\n--&gt; 332 cls = get_filesystem_class(protocol)\n    333 extra_kwargs = cls._get_kwargs_from_urls(bit)\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/fsspec/registry.py:238, in get_filesystem_class(protocol)\n    237     except ImportError as e:\n--&gt; 238         raise ImportError(bit[&quot;err&quot;]) from e\n    239 cls = registry[protocol]\n\nImportError: Please install gcsfs to access Google Storage\n\nThe above exception was the direct cause of the following exception:\n\nESMDataSourceError                        Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 dset_dict = cat.to_dataset_dict(zarr_kwargs={&#39;use_cftime&#39;:True})\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/pydantic/deprecated/decorator.py:55, in validate_arguments.&lt;locals&gt;.validate.&lt;locals&gt;.wrapper_function(*args, **kwargs)\n     53 @wraps(_func)\n     54 def wrapper_function(*args: Any, **kwargs: Any) -&gt; Any:\n---&gt; 55     return vd.call(*args, **kwargs)\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/pydantic/deprecated/decorator.py:150, in ValidatedFunction.call(self, *args, **kwargs)\n    148 def call(self, *args: Any, **kwargs: Any) -&gt; Any:\n    149     m = self.init_model_instance(*args, **kwargs)\n--&gt; 150     return self.execute(m)\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/pydantic/deprecated/decorator.py:222, in ValidatedFunction.execute(self, m)\n    220     return self.raw_function(*args_, **kwargs, **var_kwargs)\n    221 else:\n--&gt; 222     return self.raw_function(**d, **var_kwargs)\n...\n\nFile /glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/intake_esm/source.py:264, in ESMDataSource._open_dataset(self)\n    261     self._ds.attrs[OPTIONS[&#39;dataset_key&#39;]] = self.key\n    263 except Exception as exc:\n--&gt; 264     raise ESMDataSourceError(\n    265         f&quot;&quot;&quot;Failed to load dataset with key=&#39;{self.key}&#39;\n    266          You can use `cat[&#39;{self.key}&#39;].df` to inspect the assets/files for this key.\n    267          &quot;&quot;&quot;\n    268     ) from exc\n\nESMDataSourceError: Failed to load dataset with key=&#39;CMIP.NCC.NorESM2-MM.historical.Amon.gn&#39;\n                 You can use `cat[&#39;CMIP.NCC.NorESM2-MM.historical.Amon.gn&#39;].df` to inspect the assets/files for this key.\n</code></pre></div>\n<p>I've tried other environments which end up failing for different reasons. </p>\n<p>Any thoughts?</p>",
        "id": 98565,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1714587305
    },
    {
        "content": "<p>It looks like you're missing the \"gcsfs\" package for grabbing data from cloud storage.  </p>\n<p>I imagine you could make a custom environment with this dependency included, but don't know if there are any additional considerations for working with data in commercial cloud storage from the HPC systems.</p>",
        "id": 98575,
        "sender_full_name": "Katelyn FitzGerald",
        "timestamp": 1714593164
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"339\">Katelyn FitzGerald</span> <a href=\"#narrow/stream/3-general/topic/Error.20when.20loading.20CMIP6.20catalog.20with.20Pangeo/near/98575\">said</a>:</p>\n<blockquote>\n<p>It looks like you're missing the \"gcsfs\" package for grabbing data from cloud storage.  </p>\n<p>I imagine you could make a custom environment with this dependency included, but don't know if there are any additional considerations for working with data in commercial cloud storage from the HPC systems.</p>\n</blockquote>\n<p>You're right - I cloned the NPL environment and installed gcsfs and now it appears to work. Thanks <span class=\"user-mention\" data-user-id=\"339\">@Katelyn FitzGerald</span></p>",
        "id": 98590,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1714600849
    },
    {
        "content": "<p>Well another problem cropped up, further down as I try to follow through the same example as originally posted about:</p>\n<div class=\"codehilite\"><pre><span></span><code># create dictionary for reggridded data\nds_regrid_dict = dict()\nfor key in dset_dict.keys():\n    print(key)\n    ds_in = dset_dict[key]\n    ds_in = ds_in.sel(time = ds_in.time.dt.year.isin(year_range)).squeeze()\n    regridder = xe.Regridder(ds_in, ds_out, &#39;bilinear&#39;)\n    # Apply regridder to data\n    # the entire dataset can be processed at once\n    ds_in_regrid = regridder(ds_in, keep_attrs=True)\n    # Save to netcdf file\n    model = key.split(&#39;.&#39;)[2]\n    filename = &#39;tas_Amon.nc&#39;\n    savepath = &#39;CMIP6_hist/{}&#39;.format(model)\n    nc_out = os.path.join(savepath, filename)\n    os.makedirs(savepath, exist_ok=True)\n    ds_in_regrid.to_netcdf(nc_out)\n    # create dataset with all models\n    ds_regrid_dict[model] = ds_in_regrid\n    print(&#39;file written: {}&#39;.format(nc_out))\n</code></pre></div>\n<p>Gives me the following error:</p>\n<div class=\"codehilite\"><pre><span></span><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[61], line 10\n      7 regridder = xe.Regridder(ds_in, ds_out, &#39;bilinear&#39;)\n      8 # Apply regridder to data\n      9 # the entire dataset can be processed at once\n---&gt; 10 ds_in_regrid = regridder(ds_in, keep_attrs=True)\n     11 # Save to netcdf file\n     12 model = key.split(&#39;.&#39;)[2]\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xesmf/frontend.py:545, in BaseRegridder.__call__(self, indata, keep_attrs, skipna, na_thres, output_chunks)\n    537     return self.regrid_dataarray(\n    538         indata,\n    539         keep_attrs=keep_attrs,\n   (...)\n    542         output_chunks=output_chunks,\n    543     )\n    544 elif isinstance(indata, xr.Dataset):\n--&gt; 545     return self.regrid_dataset(\n    546         indata,\n    547         keep_attrs=keep_attrs,\n    548         skipna=skipna,\n    549         na_thres=na_thres,\n    550         output_chunks=output_chunks,\n    551     )\n    552 else:\n    553     raise TypeError(&#39;input must be numpy array, dask array, xarray DataArray or Dataset!&#39;)\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xesmf/frontend.py:676, in BaseRegridder.regrid_dataset(self, ds_in, keep_attrs, skipna, na_thres, output_chunks)\n    663 ds_in = ds_in.drop_vars(non_regriddable)\n    665 ds_out = xr.apply_ufunc(\n    666     self.regrid_array,\n    667     ds_in,\n   (...)\n    673     keep_attrs=keep_attrs,\n    674 )\n--&gt; 676 return self._format_xroutput(ds_out, temp_horiz_dims)\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xesmf/frontend.py:1075, in Regridder._format_xroutput(self, out, new_dims)\n   1071 if new_dims is not None:\n   1072     # rename dimension name to match output grid\n   1073     out = out.rename({nd: od for nd, od in zip(new_dims, self.out_horiz_dims)})\n-&gt; 1075 out = out.assign_coords(**self.out_coords)\n   1076 out.attrs[&#39;regrid_method&#39;] = self.method\n   1078 if self.sequence_out:\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xarray/core/common.py:621, in DataWithCoords.assign_coords(self, coords, **coords_kwargs)\n    618 else:\n    619     results = self._calc_assign_results(coords_combined)\n--&gt; 621 data.coords.update(results)\n    622 return data\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xarray/core/coordinates.py:566, in Coordinates.update(self, other)\n    560 # special case for PandasMultiIndex: updating only its dimension coordinate\n    561 # is still allowed but depreciated.\n    562 # It is the only case where we need to actually drop coordinates here (multi-index levels)\n    563 # TODO: remove when removing PandasMultiIndex&#39;s dimension coordinate.\n    564 self._drop_coords(self._names - coords_to_align._names)\n--&gt; 566 self._update_coords(coords, indexes)\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xarray/core/coordinates.py:751, in DatasetCoordinates._update_coords(self, coords, indexes)\n    748 variables.update(coords)\n    750 # check for inconsistent state *before* modifying anything in-place\n--&gt; 751 dims = calculate_dimensions(variables)\n    752 new_coord_names = set(coords)\n    753 for dim, size in dims.items():\n\nFile /glade/work/mberdahl/miniconda/envs/NPL_forCloud/lib/python3.11/site-packages/xarray/core/variable.py:3001, in calculate_dimensions(variables)\n   2999 for dim, size in zip(var.dims, var.shape):\n   3000     if dim in scalar_vars:\n-&gt; 3001         raise ValueError(\n   3002             f&quot;dimension {dim!r} already exists as a scalar variable&quot;\n   3003         )\n   3004     if dim not in dims:\n   3005         dims[dim] = size\n\nValueError: dimension &#39;member_id&#39; already exists as a scalar variable\n</code></pre></div>\n<p>I've never used regridder before - thoughts?</p>",
        "id": 98599,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1714604319
    },
    {
        "content": "<p>Just following up this issue has been resolved -- I realized that when I was creating the output grid, I had been selecting for just one member_id and then asking the regridder to apply this to models with multiple member_ids.   <br>\nFor example I did this:</p>\n<div class=\"codehilite\"><pre><span></span><code># Read in the output grid from NorESM\nds_out = ds.sel(time = ds.time.dt.year.isin(year_range), member_id=&#39;r1i1p1f1&#39;).squeeze()\n</code></pre></div>\n<p>and I should have been doing this:</p>\n<div class=\"codehilite\"><pre><span></span><code># Read in the output grid from NorESM\nds_out = ds.sel(time = ds.time.dt.year.isin(year_range)).squeeze()\n</code></pre></div>",
        "id": 98737,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1714694802
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"259\">Mira Berdahl</span> has marked this topic as resolved.</p>",
        "id": 98738,
        "sender_full_name": "Notification Bot",
        "timestamp": 1714694827
    }
]