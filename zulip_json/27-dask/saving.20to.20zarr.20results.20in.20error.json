[
    {
        "content": "<p>Hi,</p>\n<p>I'm trying to save a dataArray to a zarr file, and have been able to do this in the past with similar code.  However, lately I've run into this issue which I cannot seem to resolve.</p>\n<p>Say my DataArray looks like this: </p>\n<p>aw_oceanT_global<br>\nxarray.DataArraytime: 3600 z_t:  60<br>\n                     Array  Chunk<br>\nBytes   1.65 MiB    480 B<br>\nShape   (3600, 60)  (1, 60)<br>\nCount   154875 Tasks    3600 Chunks<br>\nType    float64 numpy.ndarray   </p>\n<p>I do :</p>\n<p>aw_oceanT_global.unify_chunks()<br>\naw_oceanT_global.to_dataset(name='TEMP').chunk({'z_t':-1}).to_zarr('/glade/scratch/mberdahl/127kaH11/TEMP/aw_oceanT_global_timeseries_H11.zarr', mode='w')</p>\n<p>But receive the following error (just the top bit of it):<br>\ndistributed.utils - ERROR - 'str' object has no attribute 'text'<br>\nTraceback (most recent call last):<br>\n  File \"/glade/work/mberdahl/miniconda/envs/pangeo/lib/python3.9/site-packages/distributed/utils.py\", line 681, in log_errors<br>\n    yield<br>\n  File \"/glade/work/mberdahl/miniconda/envs/pangeo/lib/python3.9/site-packages/distributed/dashboard/components/scheduler.py\", line 346, in update<br>\n    self.root.title.text = title<br>\nAttributeError: 'str' object has no attribute 'text'<br>\ndistributed.utils - ERROR - 'str' object has no attribute 'text'<br>\nTraceback (most recent call last):<br>\n  File \"/glade/work/mberdahl/miniconda/envs/pangeo/lib/python3.9/site-packages/distributed/utils.py\", line 681, in log_errors<br>\n    yield<br>\n  File \"/glade/work/mberdahl/miniconda/envs/pangeo/lib/python3.9/site-packages/distributed/dashboard/components/scheduler.py\", line 3417, in status_doc<br>\n    cluster_memory.update()</p>\n<p>Looks like I'm running into memory issues maybe?bBut this DataArray isn't any bigger than other arrays I've had success with with the same methods.  Any thoughts appreciated!</p>",
        "id": 49738,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643241178
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"259\">@Mira Berdahl</span>, I don't know why this is failing by looking at the traceback. Can you provide a reproducible test case or a pointer to the notebook you're using?</p>",
        "id": 49741,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643243100
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> , the path to the notebook is:<br>\n/glade/u/home/mberdahl/FirstLook_LIGruns/DASK_scripts/TryAreaWeightingAvg.ipynb<br>\nThanks for taking a look!</p>",
        "id": 49743,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643251897
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"259\">Mira Berdahl</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49743\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> , the path to the notebook is:<br>\n/glade/u/home/mberdahl/FirstLook_LIGruns/DASK_scripts/TryAreaWeightingAvg.ipynb<br>\nThanks for taking a look!</p>\n</blockquote>\n<p>Thanks, <span class=\"user-mention\" data-user-id=\"259\">@Mira Berdahl</span>! Unfortunately, I haven't been able to fix the issue (I am getting a <code>KilledWorkerError</code> when using the full dataset). In the meantime, I suggest trying the following:</p>\n<p>1) Reduce the data size you're loading (e.g. load <code>1/4</code> of it)</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">mfds</span> <span class=\"o\">=</span> <span class=\"n\">xr</span><span class=\"o\">.</span><span class=\"n\">open_mfdataset</span><span class=\"p\">(</span><span class=\"n\">dfiles</span><span class=\"p\">[:</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dfiles</span><span class=\"p\">)</span><span class=\"o\">//</span><span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"n\">combine</span><span class=\"o\">=</span><span class=\"s1\">'by_coords'</span><span class=\"p\">,</span> <span class=\"n\">parallel</span><span class=\"o\">=</span><span class=\"kc\">True</span> <span class=\"p\">,</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'time'</span><span class=\"p\">:</span> <span class=\"mi\">6</span><span class=\"p\">},</span> <span class=\"n\">data_vars</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'TEMP'</span><span class=\"p\">,</span> <span class=\"s1\">'time_bound'</span><span class=\"p\">],</span> <span class=\"n\">decode_times</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>2) Ensure the computations are done in single precision (<code>float32</code>) so as to avoid <strong>memory usage</strong> blowing up:</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">weighted_temporal_mean</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"p\">,</span> <span class=\"n\">var</span><span class=\"p\">,</span> <span class=\"n\">dims</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s1\">'nlat'</span><span class=\"p\">,</span> <span class=\"s1\">'nlon'</span><span class=\"p\">)):</span>\n    <span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">    weight by days in each month</span>\n<span class=\"sd\">    \"\"\"</span>\n    <span class=\"c1\">#time_bound_diff = ds.time_bnds.diff(dim=\"nbnds\")[:, 0]</span>\n    <span class=\"n\">month_length</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">dt</span><span class=\"o\">.</span><span class=\"n\">days_in_month</span>\n    <span class=\"n\">wgts</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">month_length</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s2\">\"time.year\"</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">month_length</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s2\">\"time.year\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">'float32'</span><span class=\"p\">)</span>\n    <span class=\"c1\">#wgts = time_bound_diff.groupby(\"time.year\") / time_bound_diff.groupby(</span>\n    <span class=\"c1\">#    \"time.year\"</span>\n    <span class=\"c1\">#).sum(xr.ALL_DIMS)</span>\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">assert_allclose</span><span class=\"p\">(</span><span class=\"n\">wgts</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s2\">\"time.year\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">xr</span><span class=\"o\">.</span><span class=\"n\">ALL_DIMS</span><span class=\"p\">),</span> <span class=\"mf\">1.0</span><span class=\"p\">)</span>\n    <span class=\"n\">obs</span> <span class=\"o\">=</span> <span class=\"n\">ds</span><span class=\"p\">[</span><span class=\"n\">var</span><span class=\"p\">]</span>\n    <span class=\"n\">cond</span> <span class=\"o\">=</span> <span class=\"n\">obs</span><span class=\"o\">.</span><span class=\"n\">isnull</span><span class=\"p\">()</span>\n    <span class=\"n\">ones</span> <span class=\"o\">=</span> <span class=\"n\">xr</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">cond</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">'float32'</span><span class=\"p\">)</span>\n    <span class=\"n\">obs_sum</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">obs</span> <span class=\"o\">*</span> <span class=\"n\">wgts</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">resample</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">=</span><span class=\"s2\">\"AS\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"s2\">\"time\"</span><span class=\"p\">)</span>\n    <span class=\"n\">ones_out</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">ones</span> <span class=\"o\">*</span> <span class=\"n\">wgts</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">resample</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">=</span><span class=\"s2\">\"AS\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"s2\">\"time\"</span><span class=\"p\">)</span>\n    <span class=\"c1\">#obs_s = (obs_sum / ones_out).mean(dims).to_series()</span>\n    <span class=\"k\">return</span> <span class=\"n\">obs_sum</span> <span class=\"o\">/</span> <span class=\"n\">ones_out</span>\n</code></pre></div>\n<p>Notice the <code>.astype('float32')</code>  in <code>weighted_temporal_mean</code> function and in the following line:</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">areacello</span> <span class=\"o\">=</span> <span class=\"n\">grid</span><span class=\"o\">.</span><span class=\"n\">TAREA</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">grid</span><span class=\"o\">.</span><span class=\"n\">KMT</span><span class=\"o\">&gt;</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">'float32'</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>3) The resultant dataset when computing the following is so tiny (~10MB) that I recommend avoiding the <code>re-chunking</code> and <code>loading</code> everything in memory before writing to disk (in <code>netCDF</code>)</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">aw_oceanT_global</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Annual_oceanT_H11</span> <span class=\"o\">*</span> <span class=\"n\">areacello</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">([</span><span class=\"s1\">'nlat'</span><span class=\"p\">,</span><span class=\"s1\">'nlon'</span><span class=\"p\">])</span><span class=\"o\">/</span><span class=\"n\">areacello</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n<span class=\"n\">aw_oceanT_global</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">()</span>\n<span class=\"n\">aw_oceanT_global</span><span class=\"o\">.</span><span class=\"n\">to_dataset</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'TEMP'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_netcdf</span><span class=\"p\">(</span><span class=\"s2\">\"............\"</span><span class=\"p\">)</span>\n</code></pre></div>",
        "id": 49839,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643650146
    },
    {
        "content": "<p>I am planning to tinker with this later today to see if I can get it to work on the full dataset. I'll keep you posted...</p>",
        "id": 49842,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643650466
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> <br>\nSorry for the delay on my end (childcare center closures have derailed me). Anyway, thanks for these suggestions. I'm able to get these methods to work while loading half the dataset. Any reason why you recommend saving as netcdf over zarr in this case?<br>\nI'll look out for any other suggestions you have for getting this working for the full dataset.  Thanks!</p>",
        "id": 49940,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643840664
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"259\">Mira Berdahl</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49940\">said</a>:</p>\n<blockquote>\n<p>Hi <span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <br>\nSorry for the delay on my end (childcare center closures have derailed me). Anyway, thanks for these suggestions. I'm able to get these methods to work while loading half the dataset. Any reason why you recommend saving as netcdf over zarr in this case?<br>\nI'll look out for any other suggestions you have for getting this working for the full dataset.  Thanks!</p>\n</blockquote>\n<p>Actually I take part of this back. I can get my notebook to work for half my dataset if I stick to saving to zarr.  The process of loading the area weighted mean and then saving to nc fails with a killedWorker error.  So seems like half is still too big for that method.</p>",
        "id": 49942,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643842601
    },
    {
        "content": "<blockquote>\n<p>Sorry for the delay on my end (childcare center closures have derailed me). Anyway, thanks for these suggestions.</p>\n</blockquote>\n<p>No worries...</p>\n<blockquote>\n<p>Actually I take part of this back. I can get my notebook to work for half my dataset if I stick to saving to zarr. The process of loading the area weighted mean and then saving to nc fails with a killedWorker error. So seems like half is still too big for that method.</p>\n</blockquote>\n<p>Interesting <span aria-label=\"thinking\" class=\"emoji emoji-1f914\" role=\"img\" title=\"thinking\">:thinking:</span>  So mysterious... Is your notebook still active? If so, could you send me the output of the following commands: </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">dask</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"jobqueue.pbs.log-directory\"</span><span class=\"p\">))</span>\n</code></pre></div>",
        "id": 49943,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643843154
    },
    {
        "content": "<p>Sure thing, it gives back the following path:<br>\n/glade/scratch/mberdahl/dask/casper-dav/logs</p>\n<p>Also having issues with getting enough workers, I request 12, but only ever receive 4 despite waiting a few hrs...</p>",
        "id": 49944,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643843547
    },
    {
        "content": "<blockquote>\n<p>Also having issues with getting enough workers, I request 12, but only ever receive 4 despite waiting a few hrs...</p>\n</blockquote>\n<p>I believe this is a CASPER issue (not sure but it appears to have a bunch of pending jobs in the queue)</p>",
        "id": 49945,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643843749
    },
    {
        "content": "<p>What's the output of </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"distributed\"</span><span class=\"p\">))</span>\n</code></pre></div>",
        "id": 49946,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643843799
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49946\">said</a>:</p>\n<blockquote>\n<p>What's the output of </p>\n<p><div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"distributed\"</span><span class=\"p\">))</span>\n</code></pre></div><br>\n</p>\n</blockquote>\n<p>This is what I see when I run that:<br>\n{'scheduler': {'bandwidth': 1000000000, 'allowed-failures': 3, 'blocked-handlers': [], 'default-data-size': '1kiB', 'events-cleanup-delay': '1h', 'idle-timeout': None, 'transition-log-length': 100000, 'events-log-length': 100000, 'work-stealing': True, 'work-stealing-interval': '100ms', 'worker-ttl': None, 'pickle': True, 'preload': [], 'preload-argv': [], 'unknown-task-duration': '500ms', 'default-task-durations': {'rechunk-split': '1us', 'split-shuffle': '1us'}, 'validate': False, 'dashboard': {'status': {'task-stream-length': 1000}, 'tasks': {'task-stream-length': 100000}, 'tls': {'ca-file': None, 'key': None, 'cert': None}, 'bokeh-application': {'allow_websocket_origin': ['*'], 'keep_alive_milliseconds': 500, 'check_unused_sessions_milliseconds': 500}}, 'locks': {'lease-validation-interval': '10s', 'lease-timeout': '30s'}, 'http': {'routes': ['distributed.http.scheduler.prometheus', '<a href=\"http://distributed.http.scheduler.info\">distributed.http.scheduler.info</a>', 'distributed.http.scheduler.json', '<a href=\"http://distributed.http.health\">distributed.http.health</a>', 'distributed.http.proxy', 'distributed.http.statics']}, 'allowed-imports': ['dask', 'distributed'], 'active-memory-manager': {'start': False, 'interval': '2s', 'policies': [{'class': 'distributed.active_memory_manager.ReduceReplicas'}]}}, 'worker': {'memory': {'target': 0.9, 'spill': False, 'pause': 0.8, 'terminate': 0.95, 'recent-to-old-time': '30s', 'rebalance': {'measure': 'optimistic', 'sender-min': 0.3, 'recipient-max': 0.6, 'sender-recipient-gap': 0.1}}, 'blocked-handlers': [], 'multiprocessing-method': 'spawn', 'use-file-locking': True, 'connections': {'outgoing': 50, 'incoming': 10}, 'preload': [], 'preload-argv': [], 'daemon': True, 'validate': False, 'resources': {}, 'lifetime': {'duration': None, 'stagger': '0 seconds', 'restart': False}, 'profile': {'interval': '10ms', 'cycle': '1000ms', 'low-level': False}, 'http': {'routes': ['distributed.http.worker.prometheus', '<a href=\"http://distributed.http.health\">distributed.http.health</a>', 'distributed.http.statics']}}, 'comm': {'compression': None, 'retry': {'count': 0, 'delay': {'min': '1s', 'max': '20s'}}, 'shard': '64MiB', 'offload': '10MiB', 'default-scheme': 'tcp', 'socket-backlog': 2048, 'recent-messages-log-length': 0, 'ucx': {'cuda-copy': None, 'tcp': None, 'nvlink': None, 'infiniband': None, 'rdmacm': None, 'net-devices': None, 'reuse-endpoints': None, 'create-cuda-context': None}, 'zstd': {'level': 3, 'threads': 0}, 'timeouts': {'connect': '30s', 'tcp': '30s'}, 'require-encryption': None, 'tls': {'ciphers': None, 'min-version': 1.2, 'max-version': None, 'ca-file': None, 'scheduler': {'cert': None, 'key': None}, 'worker': {'key': None, 'cert': None}, 'client': {'key': None, 'cert': None}}, 'tcp': {'backend': 'tornado'}, 'websockets': {'shard': '8MiB'}}, 'dashboard': {'link': '<a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/{USER}/FirstTry/proxy/{port}/status\">https://jupyterhub.hpc.ucar.edu/stable/user/{USER}/FirstTry/proxy/{port}/status</a>', 'export-tool': False, 'graph-max-items': 5000, 'prometheus': {'namespace': 'dask'}}, 'version': 2, 'nanny': {'preload': [], 'preload-argv': [], 'environ': {'MALLOC_TRIM_THRESHOLD_': 65536, 'OMP_NUM_THREADS': 1, 'MKL_NUM_THREADS': 1}}, 'client': {'heartbeat': '5s', 'scheduler-info-interval': '2s'}, 'deploy': {'lost-worker-timeout': '15s', 'cluster-repr-interval': '500ms'}, 'adaptive': {'interval': '1s', 'target-duration': '5s', 'minimum': 0, 'maximum': inf, 'wait-count': 3}, 'diagnostics': {'nvml': True, 'computations': {'max-history': 100, 'ignore-modules': ['distributed', 'dask', 'xarray', 'cudf', 'cuml', 'prefect', 'xgboost']}}, 'admin': {'tick': {'interval': '20ms', 'limit': '3s'}, 'max-error-length': 10000, 'log-length': 10000, 'log-format': '%(name)s - %(levelname)s - %(message)s', 'pdb-on-err': False, 'system-monitor': {'interval': '500ms'}, 'event-loop': 'tornado'}, 'rmm': {'pool-size': None}}</p>",
        "id": 49948,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643843858
    },
    {
        "content": "<p>I wasn't expecting it to return this much information <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span>. Try the following </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"distributed.worker.memory\"</span><span class=\"p\">)</span>\n</code></pre></div>",
        "id": 49950,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643844018
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49950\">said</a>:</p>\n<blockquote>\n<p>I wasn't expecting it to return this much information :). Try the following </p>\n<p><div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">\"distributed.worker.memory\"</span><span class=\"p\">)</span>\n</code></pre></div><br>\n</p>\n</blockquote>\n<p>haha, ok<br>\nthis looks more reasonable:<br>\n{'target': 0.9,<br>\n 'spill': False,<br>\n 'pause': 0.8,<br>\n 'terminate': 0.95,<br>\n 'recent-to-old-time': '30s',<br>\n 'rebalance': {'measure': 'optimistic',<br>\n  'sender-min': 0.3,<br>\n  'recipient-max': 0.6,<br>\n  'sender-recipient-gap': 0.1}}</p>",
        "id": 49951,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643844111
    },
    {
        "content": "<p>\\I was looking for the <code>spill</code> to disk setting and It appears to be off: <code>'spill': False,</code> ( this is the recommended option). </p>\n<p>From one of your worker logs, I am seeing that your workers are getting killed by PBS for exceeding the wall time... </p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code><span class=\"o\">=</span>&gt;&gt; PBS: job killed: walltime <span class=\"m\">7257</span> exceeded limit <span class=\"m\">7200</span>\ndistributed.dask_worker - INFO - Exiting on signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:46025'</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:39530'</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:40172'</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:33653'</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94660</span> was killed by signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94656</span> was killed by signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94663</span> was killed by signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94653</span> was killed by signal <span class=\"m\">15</span>\ndistributed.dask_worker - INFO - End worker\n</code></pre></div>\n<p>I presume these workers are getting killed while they are still trying to write data to netCDF... Is this actually the case? </p>\n<p>If Zarr is working for you, please disregard my suggestion of using netCDF... Xarray + dask can hang forever when writing data to netCDF. I don't know why but I've seen it a few times...</p>",
        "id": 49955,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643844664
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49955\">said</a>:</p>\n<blockquote>\n<p>\\I was looking for the <code>spill</code> to disk setting and It appears to be off: <code>'spill': False,</code> ( this is the recommended option). </p>\n<p>From one of your worker logs, I am seeing that your workers are getting killed by PBS for exceeding the wall time... </p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code><span class=\"o\">=</span>&gt;&gt; PBS: job killed: walltime <span class=\"m\">7257</span> exceeded limit <span class=\"m\">7200</span>\ndistributed.dask_worker - INFO - Exiting on signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:46025'</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:39530'</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:40172'</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.65:33653'</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94660</span> was killed by signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94656</span> was killed by signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94663</span> was killed by signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">94653</span> was killed by signal <span class=\"m\">15</span>\ndistributed.dask_worker - INFO - End worker\n</code></pre></div>\n<p>I presume these workers are getting killed while they are still trying to write data to netCDF... Is this actually the case? </p>\n<p>If Zarr is working for you, please disregard my suggestion of using netCDF... Xarray + dask can hang forever when writing data to netCDF. I don't know why but I've seen it a few times...</p>\n</blockquote>\n<p>OK I'll stick to zarr for now then.<br>\nIn any case, I don't think I ever got more than 4 workers to launch before the 2hr walltime was exceeded, so I think whatever is being killed never even had a chance to get going.  This has been the case for me for many days, almost a week, now...</p>",
        "id": 49956,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643844776
    },
    {
        "content": "<blockquote>\n<p>In any case, I don't think I ever got more than 4 workers to launch before the 2hr walltime was exceeded, so I think whatever is being killed never even had a chance to get going. This has been the case for me for many days, almost a week, now...</p>\n</blockquote>\n<p>Unfortunately, in some cases, the number of dask workers in your cluster matters, and you might not be able to process the data unless you have enough workers with enough resources (memory-wise). </p>\n<p>Can you try running your notebook/analysis on Cheyenne instead of Casper when you get a chance?</p>",
        "id": 49957,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643844916
    },
    {
        "content": "<p>Getting the right resources on Casper can be a headache sometimes especially when so many users are active at the same time...</p>",
        "id": 49958,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643845033
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49958\">said</a>:</p>\n<blockquote>\n<p>Getting the right resources on Casper can be a headache sometimes especially when so many users are active at the same time...</p>\n</blockquote>\n<p>OK, looks like I can get my 12 workers fairly quickly when using Cheyenne, and in so doing, load the entire dataset.  Still waiting to see if it can write to zarr or if it crashes.</p>",
        "id": 49960,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643846336
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"259\">Mira Berdahl</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49960\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/27-dask/topic/saving.20to.20zarr.20results.20in.20error/near/49958\">said</a>:</p>\n<blockquote>\n<p>Getting the right resources on Casper can be a headache sometimes especially when so many users are active at the same time...</p>\n</blockquote>\n<p>OK, looks like I can get my 12 workers fairly quickly when using Cheyenne, and in so doing, load the entire dataset.  Still waiting to see if it can write to zarr or if it crashes.</p>\n</blockquote>\n<p>It takes a while, but this runs on Cheyenne now! Many thanks for your ideas and help here <span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> .</p>",
        "id": 49962,
        "sender_full_name": "Mira Berdahl",
        "timestamp": 1643848147
    },
    {
        "content": "<p>Glad to hear it <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span> and you are welcome!</p>",
        "id": 49965,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1643899169
    }
]