[
    {
        "content": "<p>I am trying to write a script to process very-high resolution dataset (GHRSST) and when i instrument my code with memory_profiler i am seeing following statements adds extra memory consumption and i just wonder that is it possible to rewrite them to reduce the memory usage.</p>\n<p><strong>Statement 1:</strong></p>\n<div class=\"codehilite\"><pre><span></span>   97                                             # REPLACED: corner_pair_uniq = dd.from_dask_array(corner_pair).drop_duplicates().to_dask_array(lengths=True)\n    98                                             # following reduces memory by %17\n    99  258.629 MiB    0.680 MiB           1       corner_pair_uniq = dd.from_dask_array(corner_pair).drop_duplicates().values\n   100 1005.586 MiB  746.957 MiB           1       corner_pair_uniq.compute_chunk_sizes()\n</pre></div>\n\n\n<p>In this case i reduced the memory consumption by changing the calculation of corner_pair_uniq but there might be another way to reduce more.</p>\n<p><strong>Statement 2:</strong></p>\n<div class=\"codehilite\"><pre><span></span>   113 1005.586 MiB    0.000 MiB           5       corners = dd.concat([dd.from_dask_array(c) for c in [corner_lon.T.reshape((-1,)).T, corner_lat.T.reshape((-1,)).T]], axis=1)\n   114 1005.586 MiB    0.000 MiB           1       corners.columns = [&#39;lon&#39;, &#39;lat&#39;]\n   115 1789.883 MiB  784.297 MiB           1       elem_conn = corners.compute().groupby([&#39;lon&#39;,&#39;lat&#39;], sort=False).ngroup()+1\n   116 1692.887 MiB  -96.996 MiB           1       elem_conn = da.from_array(elem_conn.to_numpy())\n</pre></div>\n\n\n<p>Calculation of elem_conn introduces another jump in the memory.  Any suggestion?</p>",
        "id": 23095,
        "sender_full_name": "Ufuk Turuncoglu",
        "timestamp": 1608068569
    }
]