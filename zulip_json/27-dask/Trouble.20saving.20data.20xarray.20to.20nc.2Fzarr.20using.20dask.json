[
    {
        "content": "<p>Hello, I am having trouble saving small amounts of data (~30 kB) to nc/zarr when using dask:</p>\n<p>When I run<br>\noutput_path = '/glade/derecho/scratch/oldend/%s_UOHC_leads.nc'<br>\nens_ts.to_netcdf(output_path, unlimited_dims=['cycle'])</p>\n<p>I receive</p>\n<p>/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:3162: UserWarning: Sending large graph of size 29.59 MiB.<br>\nThis may cause some slowdown.<br>\nConsider scattering data ahead of time and using futures.<br>\nwarnings.warn(</p>\n<p>KilledWorker Traceback (most recent call last)<br>\nCell In[10], line 135<br>\n133 print('saving lead zarr')<br>\n134 output_path = '/glade/derecho/scratch/oldend/%s_UOHC_leads.nc'<br>\n--&gt; 135 ens_ts.to_netcdf(output_path, unlimited_dims=['cycle'])<br>\n136 print('ens ts saved')<br>\n137 #### Regrid OBS</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/dataarray.py:4086, in DataArray.to_netcdf(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)<br>\n4082 else:<br>\n4083 # No problems with the name - so we're fine!<br>\n4084 dataset = self.to_dataset()<br>\n-&gt; 4086 return to_netcdf( # type: ignore # mypy cannot resolve the overloads:(<br>\n4087 dataset,<br>\n4088 path,<br>\n4089 mode=mode,<br>\n4090 format=format,<br>\n4091 group=group,<br>\n4092 engine=engine,<br>\n4093 encoding=encoding,<br>\n4094 unlimited_dims=unlimited_dims,<br>\n4095 compute=compute,<br>\n4096 multifile=False,<br>\n4097 invalid_netcdf=invalid_netcdf,<br>\n4098 )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/backends/api.py:1324, in to_netcdf(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)<br>\n1321 if multifile:<br>\n1322 return writer, store<br>\n-&gt; 1324 writes = writer.sync(compute=compute)<br>\n1326 if isinstance(target, BytesIO):<br>\n1327 store.sync()</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/backends/common.py:256, in ArrayWriter.sync(self, compute, chunkmanager_store_kwargs)<br>\n253 if chunkmanager_store_kwargs is None:<br>\n254 chunkmanager_store_kwargs = {}<br>\n--&gt; 256 delayed_store = chunkmanager.store(<br>\n257 self.sources,<br>\n258 self.targets,<br>\n259 lock=self.lock,<br>\n260 compute=compute,<br>\n261 flush=True,<br>\n262 regions=self.regions,<br>\n263 **chunkmanager_store_kwargs,<br>\n264 )<br>\n265 self.sources = []<br>\n266 self.targets = []</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/daskmanager.py:233, in DaskManager.store(self, sources, targets, **kwargs)<br>\n225 def store(<br>\n226 self,<br>\n227 sources: DaskArray | Sequence[DaskArray],<br>\n228 targets: Any,<br>\n229 **kwargs,<br>\n230 ):<br>\n231 from dask.array import store<br>\n--&gt; 233 return store(<br>\n234 sources=sources,<br>\n235 targets=targets,<br>\n236 **kwargs,<br>\n237 )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/dask/array/core.py:1236, in store(failed resolving arguments)<br>\n1234 elif compute:<br>\n1235 store_dsk = HighLevelGraph(layers, dependencies)<br>\n-&gt; 1236 compute_as_if_collection(Array, store_dsk, map_keys, **kwargs)<br>\n1237 return None<br>\n1239 else:</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/dask/base.py:406, in compute_as_if_collection(cls, dsk, keys, scheduler, get, **kwargs)<br>\n404 schedule = get_scheduler(scheduler=scheduler, cls=cls, get=get)<br>\n405 dsk2 = optimization_function(cls)(dsk, keys, **kwargs)<br>\n--&gt; 406 return schedule(dsk2, keys, **kwargs)</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:3280, in Client.get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)<br>\n3278 should_rejoin = False<br>\n3279 try:<br>\n-&gt; 3280 results = self.gather(packed, asynchronous=asynchronous, direct=direct)<br>\n3281 finally:<br>\n3282 for f in futures.values():</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:2383, in Client.gather(self, futures, errors, direct, asynchronous)<br>\n2380 local_worker = None<br>\n2382 with shorten_traceback():<br>\n-&gt; 2383 return self.sync(<br>\n2384 self._gather,<br>\n2385 futures,<br>\n2386 errors=errors,<br>\n2387 direct=direct,<br>\n2388 local_worker=local_worker,<br>\n2389 asynchronous=asynchronous,<br>\n2390 )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:2243, in Client._gather(self, futures, errors, direct, local_worker)<br>\n2241 exc = CancelledError(key)<br>\n2242 else:<br>\n-&gt; 2243 raise exception.with_traceback(traceback)<br>\n2244 raise exc<br>\n2245 if errors == \"skip\":</p>\n<p>KilledWorker: Attempted to run task ('rechunk-merge-5b0135762bd66e79864c3b31aad9a747', 39, 0, 0, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://128.117.208.62:37601. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see <a href=\"https://distributed.dask.org/en/stable/killed.html\">https://distributed.dask.org/en/stable/killed.html</a>.</p>\n<p>I've attached some worker logs. Really not sure what is going on. If I try to save to zarr I get the same problem (or other errors..) I have tried adding .load() with no avail.</p>\n<p><a href=\"/user_uploads/2/2f/_DFaryQ2ajIlXDsfZRQFURJR/3872229.casper-pbs.ER\">3872229.casper-pbs.ER</a><br>\n<a href=\"/user_uploads/2/b8/vjepGSwIvSXAgwxr5MaCtIaL/3872198.casper-pbs.ER\">3872198.casper-pbs.ER</a></p>\n<p>Thanks,</p>\n<p>Dylan</p>",
        "id": 105654,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740415117
    },
    {
        "content": "<p>How are you creating <code>ens_to</code>? I wonder if the problem is in the setup of the dataset, and the <code>to_netcdf</code> call is the first time dask is trying to run through the whole graph. If you have <code>xr.open_mfdataset()</code> or <code>xr.concat()</code> calls, using <code>compat='override'</code> and <code>join='override'</code> will remove the consistency checks that verify all the coordinate arrays are the same across every file / dataset.</p>",
        "id": 105668,
        "sender_full_name": "Michael Levy",
        "timestamp": 1740424513
    },
    {
        "content": "<p>Thanks. I was using xr.open_mfdataset (already with compat='override' and join='override') as well as xr.concat (without those). I've added ens_ts0 = xr.concat([ens_ts0,ens_ts],dim='L',compat=\"override\",join=\"override\") but now I receive this error when doing xr.concat:</p>\n<hr>\n<p>ValueError                                Traceback (most recent call last)<br>\nCell In[8], line 127<br>\n    125         ens_time_year = dp2_time.isel(L=lvals+i).mean('L')<br>\n    126         ens_ts = ens_ts.assign_coords(time=(\"time\",ens_time_year.data)).expand_dims('L')<br>\n--&gt; 127         ens_ts0 = xr.concat([ens_ts0,ens_ts],dim='L',compat=\"override\",join=\"override\")<br>\n    128 ens_ts = ens_ts0.copy()<br>\n    129 del ens_ts0</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/concat.py:264, in concat(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)<br>\n    259     raise ValueError(<br>\n    260         f\"compat={compat!r} invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'\"<br>\n    261     )<br>\n    263 if isinstance(first_obj, DataArray):<br>\n--&gt; 264     return _dataarray_concat(<br>\n    265         objs,<br>\n    266         dim=dim,<br>\n    267         data_vars=data_vars,<br>\n    268         coords=coords,<br>\n    269         compat=compat,<br>\n    270         positions=positions,<br>\n    271         fill_value=fill_value,<br>\n    272         join=join,<br>\n    273         combine_attrs=combine_attrs,<br>\n    274         create_index_for_new_dim=create_index_for_new_dim,<br>\n    275     )<br>\n    276 elif isinstance(first_obj, Dataset):<br>\n    277     return _dataset_concat(<br>\n    278         objs,<br>\n    279         dim=dim,<br>\n   (...)<br>\n    287         create_index_for_new_dim=create_index_for_new_dim,<br>\n    288     )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/concat.py:755, in _dataarray_concat(arrays, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)<br>\n    752             arr = arr.rename(name)<br>\n    753     datasets.append(arr._to_temp_dataset())<br>\n--&gt; 755 ds = _dataset_concat(<br>\n    756     datasets,<br>\n    757     dim,<br>\n    758     data_vars,<br>\n    759     coords,<br>\n    760     compat,<br>\n    761     positions,<br>\n    762     fill_value=fill_value,<br>\n    763     join=join,<br>\n    764     combine_attrs=combine_attrs,<br>\n    765     create_index_for_new_dim=create_index_for_new_dim,<br>\n    766 )<br>\n    768 merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)<br>\n    770 result = arrays[0]._from_temp_dataset(ds, name)</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/concat.py:545, in _dataset_concat(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)<br>\n    539     datasets = [<br>\n    540         ds.expand_dims(dim_name, create_index_for_new_dim=create_index_for_new_dim)<br>\n    541         for ds in datasets<br>\n    542     ]<br>\n    544 # determine which variables to concatenate<br>\n--&gt; 545 concat_over, equals, concat_dim_lengths = _calc_concat_over(<br>\n    546     datasets, dim_name, dim_names, data_vars, coords, compat<br>\n    547 )<br>\n    549 # determine which variables to merge, and then merge them according to compat<br>\n    550 variables_to_merge = (coord_names | data_names) - concat_over</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/concat.py:440, in _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat)<br>\n    437         concat_over.update(opt)<br>\n    439 process_subset_opt(data_vars, \"data_vars\")<br>\n--&gt; 440 process_subset_opt(coords, \"coords\")<br>\n    441 return concat_over, equals, concat_dim_lengths</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/concat.py:350, in _calc_concat_over.&lt;locals&gt;.process_subset_opt(opt, subset)<br>\n    348 if opt == \"different\":<br>\n    349     if compat == \"override\":<br>\n--&gt; 350         raise ValueError(<br>\n    351             f\"Cannot specify both {subset}='different' and compat='override'.\"<br>\n    352         )<br>\n    353     # all nonindexes that are not the same in each dataset<br>\n    354     for k in getattr(datasets[0], subset):</p>\n<p>ValueError: Cannot specify both coords='different' and compat='override'.<br>\nens_ts</p>",
        "id": 105669,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740426078
    },
    {
        "content": "<p>This is how ens_ts is computed:<br>\nlvals = np.arange(nyear)<br>\n    lvalsda = xr.DataArray(np.arange(nleads),dims=\"L\",name=\"L\")<br>\n    for i in range(nleads):<br>\n        if i==range(nleads)[0]:<br>\n            ens_ts0 = ohc_anom.isel(L=lvals+i).mean('L').rename({'Y':'time'}).chunk(dict(time=-1))<br>\n            ens_time_year = dp2_time.isel(L=lvals+i).mean('L')<br>\n            ens_ts0 = ens_ts0.assign_coords(time=(\"time\",ens_time_year.data)).expand_dims('L')<br>\n        else:<br>\n            ens_ts = ohc_anom.isel(L=lvals+i).mean('L').rename({'Y':'time'}).chunk(dict(time=-1))<br>\n            ens_time_year = dp2_time.isel(L=lvals+i).mean('L')<br>\n            ens_ts = ens_ts.assign_coords(time=(\"time\",ens_time_year.data)).expand_dims('L')<br>\n            ens_ts0 = xr.concat([ens_ts0,ens_ts],dim='L',compat=\"override\",join=\"override\")<br>\n    ens_ts = ens_ts0.copy()<br>\n    del ens_ts0</p>",
        "id": 105670,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740426153
    },
    {
        "content": "<blockquote>\n<p>I've added ens_ts0 = xr.concat([ens_ts0,ens_ts],dim='L',compat=\"override\",join=\"override\") but now I receive this error when doing xr.concat:</p>\n</blockquote>\n<p>Can you add <code>coords = \"minimal\"</code> as well? You can read about these options on the <a href=\"https://docs.xarray.dev/en/stable/generated/xarray.concat.html\"><code>xr.concat()</code></a> documentation page, but I think this change (combined with <code>compat=\"override\"</code>) will assume that coordinates are the same across all ensemble members</p>",
        "id": 105671,
        "sender_full_name": "Michael Levy",
        "timestamp": 1740431248
    },
    {
        "content": "<p>That does make xr.concat() work, but then when I try to do ens_ts.to_netcdf(output_path) then I get:</p>\n<p>CancelledError                            Traceback (most recent call last)<br>\nCell In[7], line 138<br>\n    135 print('saving lead')<br>\n    137 output_path = '/glade/derecho/scratch/oldend/%s_UOHC_leads.nc' % model<br>\n--&gt; 138 ens_ts.to_netcdf(output_path)#, unlimited_dims=['cycle'])<br>\n    140 print('ens ts saved')<br>\n    141 #### Regrid OBS</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/dataarray.py:4211, in DataArray.to_netcdf(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)<br>\n   4207 else:<br>\n   4208     # No problems with the name - so we're fine!<br>\n   4209     dataset = self.to_dataset()<br>\n-&gt; 4211 return to_netcdf(  # type: ignore[return-value]  # mypy cannot resolve the overloads:(<br>\n   4212     dataset,<br>\n   4213     path,<br>\n   4214     mode=mode,<br>\n   4215     format=format,<br>\n   4216     group=group,<br>\n   4217     engine=engine,<br>\n   4218     encoding=encoding,<br>\n   4219     unlimited_dims=unlimited_dims,<br>\n   4220     compute=compute,<br>\n   4221     multifile=False,<br>\n   4222     invalid_netcdf=invalid_netcdf,<br>\n   4223     auto_complex=auto_complex,<br>\n   4224 )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/backends/api.py:1882, in to_netcdf(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)<br>\n   1879 if multifile:<br>\n   1880     return writer, store<br>\n-&gt; 1882 writes = writer.sync(compute=compute)<br>\n   1884 if isinstance(target, BytesIO):<br>\n   1885     store.sync()</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/backends/common.py:351, in ArrayWriter.sync(self, compute, chunkmanager_store_kwargs)<br>\n    348 if chunkmanager_store_kwargs is None:<br>\n    349     chunkmanager_store_kwargs = {}<br>\n--&gt; 351 delayed_store = chunkmanager.store(<br>\n    352     self.sources,<br>\n    353     self.targets,<br>\n    354     lock=self.lock,<br>\n    355     compute=compute,<br>\n    356     flush=True,<br>\n    357     regions=self.regions,<br>\n    358     **chunkmanager_store_kwargs,<br>\n    359 )<br>\n    360 self.sources = []<br>\n    361 self.targets = []</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py:247, in DaskManager.store(self, sources, targets, **kwargs)<br>\n    239 def store(<br>\n    240     self,<br>\n    241     sources: Any | Sequence[Any],<br>\n    242     targets: Any,<br>\n    243     **kwargs: Any,<br>\n    244 ) -&gt; Any:<br>\n    245     from dask.array import store<br>\n--&gt; 247     return store(<br>\n    248         sources=sources,<br>\n    249         targets=targets,<br>\n    250         **kwargs,<br>\n    251     )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/dask/array/core.py:1236, in store(<strong><em>failed resolving arguments</em></strong>)<br>\n   1234 elif compute:<br>\n   1235     store_dsk = HighLevelGraph(layers, dependencies)<br>\n-&gt; 1236     compute_as_if_collection(Array, store_dsk, map_keys, **kwargs)<br>\n   1237     return None<br>\n   1239 else:</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/dask/base.py:402, in compute_as_if_collection(cls, dsk, keys, scheduler, get, **kwargs)<br>\n    400 schedule = get_scheduler(scheduler=scheduler, cls=cls, get=get)<br>\n    401 dsk2 = optimization_function(cls)(dsk, keys, **kwargs)<br>\n--&gt; 402 return schedule(dsk2, keys, **kwargs)</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:3279, in Client.get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)<br>\n   3277         should_rejoin = False<br>\n   3278 try:<br>\n-&gt; 3279     results = self.gather(packed, asynchronous=asynchronous, direct=direct)<br>\n   3280 finally:<br>\n   3281     for f in futures.values():</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:2372, in Client.gather(self, futures, errors, direct, asynchronous)<br>\n   2369     local_worker = None<br>\n   2371 with shorten_traceback():<br>\n-&gt; 2372     return self.sync(<br>\n   2373         self._gather,<br>\n   2374         futures,<br>\n   2375         errors=errors,<br>\n   2376         direct=direct,<br>\n   2377         local_worker=local_worker,<br>\n   2378         asynchronous=asynchronous,<br>\n   2379     )</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:2233, in Client._gather(self, futures, errors, direct, local_worker)<br>\n   2231     else:<br>\n   2232         raise exception.with_traceback(traceback)<br>\n-&gt; 2233     raise exc<br>\n   2234 if errors == \"skip\":<br>\n   2235     bad_keys.add(key)</p>\n<p>CancelledError: ('store-map-0b438090245098763a0c364fab4fea00', 0, 5, 0)</p>",
        "id": 105672,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740432266
    },
    {
        "content": "<p>A couple of thoughts that might help debug:<br>\n— When you say adding a .load() doesn't fix the problem, does the .load() on its own throw the error, or the saving to netcdf after loading the file? This might narrow down whether it's a netcdf/saving problem or an issue with the dask graph. <br>\n— Does halving what you're computing or making smaller chunk sizes help? In the extreme case, is there some trivial/smallest subset of what you're trying to calculate that works? Or, in the other direction, what happens if you double or triple the memory but keep the number of workers the same?<br>\n— Can you open the dask homepage and watch the memory levels or taskstream just before it crashes? Does it do some computing, then sort-of stop and one worker's memory skyrocket?</p>\n<p>Based on the error you're receiving and what you've described, it seems likely to me that this is not a problem directly related to saving, but instead an error when loading/calculating the thing you want to save. In which case, whatever is causing the error could be code from well before the code that throws the error, that's embedding instructions into the dask graph that's only calculated later</p>",
        "id": 105679,
        "sender_full_name": "Jemma Jeffree",
        "timestamp": 1740523253
    },
    {
        "content": "<p>Doing the load() by itself throws an error:</p>\n<hr>\n<p>CancelledError                            Traceback (most recent call last)<br>\nCell In[19], line 1<br>\n----&gt; 1 ens_ts = ens_ts.load()</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/dataarray.py:1175, in DataArray.load(self, **kwargs)<br>\n   1155 def load(self, **kwargs) -&gt; Self:<br>\n   1156     \"\"\"Manually trigger loading of this array's data from disk or a<br>\n   1157     remote source into memory and return this array.<br>\n   1158 <br>\n   (...)<br>\n   1173     dask.compute<br>\n   1174     \"\"\"<br>\n-&gt; 1175     ds = self._to_temp_dataset().load(**kwargs)<br>\n   1176     new = self._from_temp_dataset(ds)<br>\n   1177     self._variable = new._variable</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/core/dataset.py:899, in Dataset.load(self, **kwargs)<br>\n    896 chunkmanager = get_chunked_array_type(*lazy_data.values())<br>\n    898 # evaluate all the chunked arrays simultaneously<br>\n--&gt; 899 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(<br>\n    900     *lazy_data.values(), **kwargs<br>\n    901 )<br>\n    903 for k, data in zip(lazy_data, evaluated_data, strict=False):<br>\n    904     self.variables[k].data = data</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)<br>\n     80 def compute(<br>\n     81     self, *data: Any, **kwargs: Any<br>\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:<br>\n     83     from dask.array import compute<br>\n---&gt; 85     return compute(*data, **kwargs)</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/dask/base.py:661, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)<br>\n    658     postcomputes.append(x.__dask_postcompute__())<br>\n    660 with shorten_traceback():<br>\n--&gt; 661     results = schedule(dsk, keys, **kwargs)<br>\n    663 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])</p>\n<p>File ~/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/client.py:2233, in Client._gather(self, futures, errors, direct, local_worker)<br>\n   2231     else:<br>\n   2232         raise exception.with_traceback(traceback)<br>\n-&gt; 2233     raise exc<br>\n   2234 if errors == \"skip\":<br>\n   2235     bad_keys.add(key)</p>\n<p>CancelledError: ('transpose-964b93e84eb977cb7290c56ee6ec09a5', 0, 4, 6)</p>\n<p>It shows a lot of movement during the previous parts of the code, then when it gets to the to_netcdf part, it just stays like this, then remains like it before/after the error shows up (see attached)<br>\n<a href=\"/user_uploads/2/68/s1XLNoZT8kfub5F2KSMKhGwB/Capture-décran-2025-02-25-à-16.31.42.png\">Capture-décran-2025-02-25-à-16.31.42.png</a><br>\n<a href=\"/user_uploads/2/96/u6rvIruNBdLauR_xSOsUIxWW/Capture-décran-2025-02-25-à-16.30.39.png\">Capture-décran-2025-02-25-à-16.30.39.png</a><br>\n<a href=\"/user_uploads/2/29/JVzf_UwJwieGUxaTII7LyvvX/Capture-décran-2025-02-25-à-16.30.18.png\">Capture-décran-2025-02-25-à-16.30.18.png</a><br>\nThe workers no longer crash, but it throws this strange CancelledError now. I've attached some worker logs<br>\n<a href=\"/user_uploads/2/aa/oGEssRBHM6nv9aweQo3SCIZa/3890166.casper-pbs.ER\">3890166.casper-pbs.ER</a><br>\n<a href=\"/user_uploads/2/e5/V0PK-2pU2RDuWog0pVJOeKYO/3890165.casper-pbs.ER\">3890165.casper-pbs.ER</a><br>\n<a href=\"/user_uploads/2/41/AuU6bR3iKTD_Fq-0aYB7BUOi/3890161.casper-pbs.ER\">3890161.casper-pbs.ER</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/68/s1XLNoZT8kfub5F2KSMKhGwB/Capture-décran-2025-02-25-à-16.31.42.png\" title=\"Capture-décran-2025-02-25-à-16.31.42.png\"><img src=\"/user_uploads/2/68/s1XLNoZT8kfub5F2KSMKhGwB/Capture-décran-2025-02-25-à-16.31.42.png\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/2/96/u6rvIruNBdLauR_xSOsUIxWW/Capture-décran-2025-02-25-à-16.30.39.png\" title=\"Capture-décran-2025-02-25-à-16.30.39.png\"><img src=\"/user_uploads/2/96/u6rvIruNBdLauR_xSOsUIxWW/Capture-décran-2025-02-25-à-16.30.39.png\"></a></div><div class=\"message_inline_image\"><a href=\"/user_uploads/2/29/JVzf_UwJwieGUxaTII7LyvvX/Capture-décran-2025-02-25-à-16.30.18.png\" title=\"Capture-décran-2025-02-25-à-16.30.18.png\"><img src=\"/user_uploads/2/29/JVzf_UwJwieGUxaTII7LyvvX/Capture-décran-2025-02-25-à-16.30.18.png\"></a></div>",
        "id": 105680,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740530384
    },
    {
        "content": "<p>The \"CancelledError\" is the new problem. Previously it seemed like there was a memory overload for the workers, but now that does not seem to be the problem.</p>",
        "id": 105681,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740530725
    },
    {
        "content": "<p>The errors changed when I updated all my packages and my conda version. I used to be able to plot these arrays and load() used to work, allowing me to call the values. Now none of it works at all.</p>",
        "id": 105682,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740530788
    },
    {
        "content": "<p>Also, when the CancelledError appears, an error appears below where I call the dask client which is too long to even include here, but includes:</p>\n<p>2025-02-25 17:42:54,603 - distributed.protocol.core - CRITICAL - Failed to Serialize<br>\nTraceback (most recent call last):<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/protocol/core.py\", line 109, in dumps<br>\n    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)<br>\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/__init__.py\", line 35, in packb<br>\n    return Packer(**kwargs).pack(o)<br>\n           ^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 885, in pack<br>\n    self._pack(obj)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 864, in _pack<br>\n    return self._pack_map_pairs(<br>\n           ^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 970, in _pack_map_pairs<br>\n    self._pack(v, nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 819, in _pack<br>\n    n = len(obj) * obj.itemsize<br>\n        ^^^^^^^^<br>\nTypeError: 0-dim memory has no length</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>Traceback (most recent call last):<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/protocol/core.py\", line 130, in dumps<br>\n    frames[0] = msgpack.dumps(<br>\n                ^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/__init__.py\", line 35, in packb<br>\n    return Packer(**kwargs).pack(o)<br>\n           ^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 885, in pack<br>\n    self._pack(obj)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 864, in _pack<br>\n    return self._pack_map_pairs(<br>\n           ^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 970, in _pack_map_pairs<br>\n    self._pack(v, nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 819, in _pack<br>\n    n = len(obj) * obj.itemsize<br>\n        ^^^^^^^^<br>\nTypeError: 0-dim memory has no length<br>\n2025-02-25 17:42:54,608 - distributed.comm.utils - ERROR - 0-dim memory has no length<br>\nTraceback (most recent call last):<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/protocol/core.py\", line 109, in dumps<br>\n    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)<br>\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/__init__.py\", line 35, in packb<br>\n    return Packer(**kwargs).pack(o)<br>\n           ^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 885, in pack<br>\n    self._pack(obj)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 864, in _pack<br>\n    return self._pack_map_pairs(<br>\n           ^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 970, in _pack_map_pairs<br>\n    self._pack(v, nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 819, in _pack<br>\n    n = len(obj) * obj.itemsize<br>\n        ^^^^^^^^<br>\nTypeError: 0-dim memory has no length</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>Traceback (most recent call last):<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/comm/utils.py\", line 34, in _to_frames<br>\n    return list(protocol.dumps(msg, **kwargs))<br>\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/protocol/core.py\", line 130, in dumps<br>\n    frames[0] = msgpack.dumps(<br>\n                ^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/__init__.py\", line 35, in packb<br>\n    return Packer(**kwargs).pack(o)<br>\n           ^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 885, in pack<br>\n    self._pack(obj)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 864, in _pack<br>\n    return self._pack_map_pairs(<br>\n           ^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 970, in _pack_map_pairs<br>\n    self._pack(v, nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 819, in _pack<br>\n    n = len(obj) * obj.itemsize<br>\n        ^^^^^^^^<br>\nTypeError: 0-dim memory has no length<br>\n2025-02-25 17:42:54,611 - distributed.batched - ERROR - Error in batched write<br>\nTraceback (most recent call last):<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/distributed/protocol/core.py\", line 109, in dumps<br>\n    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)<br>\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/__init__.py\", line 35, in packb<br>\n    return Packer(**kwargs).pack(o)<br>\n           ^^^^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 885, in pack<br>\n    self._pack(obj)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 864, in _pack<br>\n    return self._pack_map_pairs(<br>\n           ^^^^^^^^^^^^^^^^^^^^^<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 970, in _pack_map_pairs<br>\n    self._pack(v, nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 861, in _pack<br>\n    self._pack(obj[i], nest_limit - 1)<br>\n  File \"/glade/u/home/oldend/anaconda3/envs/derecho/lib/python3.12/site-packages/msgpack/fallback.py\", line 819, in _pack<br>\n    n = len(obj) * obj.itemsize<br>\n        ^^^^^^^^<br>\nTypeError: 0-dim memory has no length</p>\n<p>It seems like basically every single part of this code is failing.</p>",
        "id": 105683,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740531193
    },
    {
        "content": "<p>It turns out the problem was with my python environment - when I switch to the built-in NPL 2025a conda environment, everything works. I have no idea why.</p>",
        "id": 105684,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740543329
    },
    {
        "content": "<p>Well, not quite true. doing ens_ts = ens_ts.load() gives me a netCDF HDI error, but I can save the file.</p>",
        "id": 105685,
        "sender_full_name": "Dylan Oldenburg",
        "timestamp": 1740543366
    }
]