[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"176\">@Maria Molina</span> and I are working on  a notebook to extract data from the CESM2-LE for self organizing maps. We're using the intake-esm example and therefore dask. We keep getting an error about killed workers in DASK.  (see below for error). We're unsure the solution. We've tried doing some of the calculations earlier (e.g. a .persist earlier in the notebook) but maybe the solution is we need more workers or memory or cores. Is there any guidance for how to navigate this sort of error in DASK? <span class=\"user-mention\" data-user-id=\"134\">@Max Grover</span> <span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> , you may have some DASK specific guidance as well and I've worked with both of you on aspects of this notebook already.</p>\n<p>KilledWorker: ('open_dataset-10b51ac878a730e7158c2de0c868d102coast_mask-dc1e9cae81941bac9542c4296cdde25b', &lt;WorkerState 'tcp://10.12.206.8:43462', name: PBSCluster-9, memory: 0, processing: 44&gt;)</p>",
        "id": 41115,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1630017057
    },
    {
        "content": "<p>Unfortunately, there typically is not an easy answer to that... you can try allocating more memory/workers, checking the size of your chunks, and breaking the computation into smaller portions.  We put together <em>some</em> documentation on this on the <a href=\"https://ncar.github.io/esds/faq/#xarray-and-dask\">ESDS FAQ page</a>, but this may not completely solve your problem. I encourage you stop by our office hours on Monday, we can try taking a look at the Dask dashboard to see if we can diagnose the problem if that works?</p>",
        "id": 41116,
        "sender_full_name": "Max Grover",
        "timestamp": 1630075676
    },
    {
        "content": "<p>Okay, I'll see how far I can get today with the FAQ, but otherwise I'll see you again at office hours next week. Thanks! :)</p>",
        "id": 41140,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1630078077
    },
    {
        "content": "<p>congratulations! you have found an xarray <a href=\"https://github.com/pydata/xarray/issues/5754\">bug</a> =)</p>\n<p>When you stack <code>member_id</code> and <code>time</code>. into a new dimension it consolidated that into one giant 8GB chunk, which causes all the trouble. Good news is that if you subset in latitude early enough (since most of the domain is useless for what you want to do) everything works perfectly fine on my end.</p>\n<p>Here's the notebook with some notes and suggestions: <a href=\"/user_uploads/2/2/xX9W5Crm6H1F8_3SdSkhK6px/soms_antarctica-gettingdata.ipynb\">soms_antarctica-gettingdata.ipynb</a></p>",
        "id": 41567,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1630465764
    },
    {
        "content": "<p>Awesome!! Thanks <span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> !</p>",
        "id": 41568,
        "sender_full_name": "Maria Molina",
        "timestamp": 1630469042
    },
    {
        "content": "<p>Thanks! I will give that a try. :)</p>",
        "id": 41632,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1630533052
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> I downloaded the notebook that appears to be attached to your message, but I looked through and don't see the notes or suggestions (I also diffed it with the notebook I sent you and the diff came back with no changes). Am I missing something obvious here?</p>",
        "id": 41755,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1630604523
    },
    {
        "content": "<p>ouch it didn't save in that case (the notebook was open read-only in your directory, i assumed that it would add my changes when I clicked download).</p>\n<p>IIRC the main suggestion was to use <code>ds.isel(time=ds.time.dt.month.isin([10, 11, 12]))</code> to pick out OND months.</p>\n<p>I also added <code>compat=\"override\"</code> in various <code>open_mfdataset</code> / <code>concat</code> lines to prevent constructing giant 5D TAREA arrays though I don't think that affected the final computation</p>",
        "id": 41762,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1630608713
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> I just got a chance to try your suggestions, but just adding the ds.isel(time=ds.time.dt.mont.isin([4,5,6,7,8,9,10])) wasn't enough to prevent all the killed workers. Would you mind coming to office hours on Monday so you and I can talk about it again? Or if that time doesn't work for you, maybe we could find another time?</p>",
        "id": 42401,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1631302477
    },
    {
        "content": "<p>For the killed workers, you should subset in latitude early (before stacking) since most of the domain is useless for what you were trying.</p>",
        "id": 42411,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1631307870
    },
    {
        "content": "<p>Update: I added a .persist() right before I did the big calculation and it hasn't died (yet). I'm keeping my fingers crossed...</p>\n<p>I tried dropping the unused horizontal values a lot sooner and then subsetting time, all before stacking, and I still ended up with the Killed Worker error. Were you able to get it to work in your testing? If so, I wonder if I'm still missing some crucial piece.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"25\">Deepak Cherian</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/42411\">said</a>:</p>\n<blockquote>\n<p>For the killed workers, you should subset in latitude early (before stacking) since most of the domain is useless for what you were trying.</p>\n</blockquote>",
        "id": 42420,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1631313097
    },
    {
        "content": "<p>Just before section 4, I executed <code>ds_ice_winter.load()</code> (I chose to do this because that array is now only 1GB) and it works fine with 10 workers and 15GB memory each.<br>\n<a href=\"/user_uploads/2/77/AteziURinWB1PVa6sQJ-9Y6o/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/77/AteziURinWB1PVa6sQJ-9Y6o/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/77/AteziURinWB1PVa6sQJ-9Y6o/image.png\"></a></div><p>I did skip this cell which seems useless? It doesn't seem to drop anything.</p>\n<div class=\"codehilite\"><pre><span></span><code>ds_ice_masked_subset = ds_ice_masked_subset.where(ds_ice_masked_subset!=-999.999, drop=True)\n</code></pre></div>\n<p>I recommend updating dask and distributed (i'm running <code>2021.7.2</code>). There've been a lot of updates recently that apparently help with reducing memory usage. Maybe one of those improvements has drastically improved this sequence of operations.</p>\n<div class=\"codehilite\"><pre><span></span><code>numpy      : 1.21.1\ndistributed: 2021.7.2\ncartopy    : 0.19.0.post1\nmatplotlib : 3.4.2\nintake     : 0.6.2\nxarray     : 0.19.0\npandas     : 1.3.1\ncftime     : 1.5.0\ndask       : 2021.7.2\n</code></pre></div>",
        "id": 42434,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1631547644
    },
    {
        "content": "<p>Ok, I was able to load the data this way and not kill all the dask workers. So that's a success. However, when I move on to the next step I remembered the reason I'd originally gone in the order of operations that created the Dask problem. There is an issue with xarray with dropping nans - basically it doesn't work. And the SOM algorithm can't have nans, so we have to figure out a way to drop them.</p>\n<p>As it stands now, I have an array ds_ice_winter with shape (25 -members, 21614 - time, 14 - nj, 35 - ni) that is significantly reduced in 2D size from the global data. Here's the first timestep, basically all the purple is a nan currently and this is invariant in time. All those values should be dropped so the SOM doesn't train with them.<br>\n<a href=\"/user_uploads/2/30/VR3AvK8kY_cp3wrxAWGG9sS9/Screen-Shot-2021-09-14-at-3.43.24-PM.png\">Screen-Shot-2021-09-14-at-3.43.24-PM.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/30/VR3AvK8kY_cp3wrxAWGG9sS9/Screen-Shot-2021-09-14-at-3.43.24-PM.png\" title=\"Screen-Shot-2021-09-14-at-3.43.24-PM.png\"><img src=\"/user_uploads/2/30/VR3AvK8kY_cp3wrxAWGG9sS9/Screen-Shot-2021-09-14-at-3.43.24-PM.png\"></a></div><p>So I stack the horizontal points and then the members and replace all the nans with a fill number. This seems to work. I can see that for temp2 there are -999.99 values but I have checked and not <em>all</em> the values are the missing.<br>\n<a href=\"/user_uploads/2/8f/_MeLLg2PialbGjeO2lu3_lu5/Screen-Shot-2021-09-14-at-3.48.37-PM.png\">Screen-Shot-2021-09-14-at-3.48.37-PM.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/8f/_MeLLg2PialbGjeO2lu3_lu5/Screen-Shot-2021-09-14-at-3.48.37-PM.png\" title=\"Screen-Shot-2021-09-14-at-3.48.37-PM.png\"><img src=\"/user_uploads/2/8f/_MeLLg2PialbGjeO2lu3_lu5/Screen-Shot-2021-09-14-at-3.48.37-PM.png\"></a></div><p>However, when I try to drop the values that are equal to -999.99, it doesn't seem to work and just returns an array with all the nans back. <br>\n<a href=\"/user_uploads/2/39/dtLzwOaQGmSvN2nMrFzd_jqc/Screen-Shot-2021-09-14-at-3.48.47-PM.png\">Screen-Shot-2021-09-14-at-3.48.47-PM.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/39/dtLzwOaQGmSvN2nMrFzd_jqc/Screen-Shot-2021-09-14-at-3.48.47-PM.png\" title=\"Screen-Shot-2021-09-14-at-3.48.47-PM.png\"><img src=\"/user_uploads/2/39/dtLzwOaQGmSvN2nMrFzd_jqc/Screen-Shot-2021-09-14-at-3.48.47-PM.png\"></a></div><p><span class=\"user-mention\" data-user-id=\"176\">@Maria Molina</span> and I have both worked on this and can't figure out a way to replace the nans in a way that they will be dropped by xarray. Xarray seems to remember from way back with the masking by the location that these are missing and won't let them go. </p>\n<p>Any suggestions? We're stumped.</p>",
        "id": 42720,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1631656345
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span>,</p>\n<blockquote>\n<p>However, when I try to drop the values that are equal to -999.99, it doesn't seem to work and just returns an array with all the nans back. </p>\n</blockquote>\n<p>Consider this array</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">57</span><span class=\"p\">]:</span> <span class=\"n\">array</span> <span class=\"o\">=</span> <span class=\"n\">xr</span><span class=\"o\">.</span><span class=\"n\">DataArray</span><span class=\"p\">([[</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]],</span> <span class=\"n\">dims</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'x'</span><span class=\"p\">,</span><span class=\"s1\">'y'</span><span class=\"p\">],</span> <span class=\"n\">coords</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'x'</span><span class=\"p\">:</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)})</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">58</span><span class=\"p\">]:</span> <span class=\"n\">array</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">58</span><span class=\"p\">]:</span>\n<span class=\"o\">&lt;</span><span class=\"n\">xarray</span><span class=\"o\">.</span><span class=\"n\">DataArray</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">&gt;</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">]])</span>\n<span class=\"n\">Coordinates</span><span class=\"p\">:</span>\n  <span class=\"o\">*</span> <span class=\"n\">x</span>        <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"n\">int64</span> <span class=\"mi\">0</span> <span class=\"mi\">1</span>\n<span class=\"n\">Dimensions</span> <span class=\"n\">without</span> <span class=\"n\">coordinates</span><span class=\"p\">:</span> <span class=\"n\">y</span>\n</code></pre></div>\n<p>Masking zeros results in this array... </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">59</span><span class=\"p\">]:</span> <span class=\"n\">array</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">array</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">59</span><span class=\"p\">]:</span>\n<span class=\"o\">&lt;</span><span class=\"n\">xarray</span><span class=\"o\">.</span><span class=\"n\">DataArray</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">&gt;</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">1.</span><span class=\"p\">,</span>  <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"n\">nan</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span><span class=\"n\">nan</span><span class=\"p\">,</span>  <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"n\">nan</span><span class=\"p\">]])</span>\n<span class=\"n\">Coordinates</span><span class=\"p\">:</span>\n  <span class=\"o\">*</span> <span class=\"n\">x</span>        <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"n\">int64</span> <span class=\"mi\">0</span> <span class=\"mi\">1</span>\n<span class=\"n\">Dimensions</span> <span class=\"n\">without</span> <span class=\"n\">coordinates</span><span class=\"p\">:</span> <span class=\"n\">y</span>\n</code></pre></div>\n<p>Notice the location of the <code>NaNs</code>.  Assuming we want to preserve the dimensionality of our original array, dropping all NaNs in this 2D array would leave us with an <strong>awkward</strong> array. To drop everything we would have to collapse the result to a 1D array. Because <code>xr.where(.... drop=True)</code> preserves the <strong>dimensionality</strong> of the array, it drops as many values as possible while setting data values to <code>NaN</code> where necessary. </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">60</span><span class=\"p\">]:</span> <span class=\"n\">array</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">array</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">60</span><span class=\"p\">]:</span>\n<span class=\"o\">&lt;</span><span class=\"n\">xarray</span><span class=\"o\">.</span><span class=\"n\">DataArray</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">&gt;</span>\n<span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">1.</span><span class=\"p\">,</span>  <span class=\"mf\">1.</span><span class=\"p\">],</span>\n       <span class=\"p\">[</span><span class=\"n\">nan</span><span class=\"p\">,</span>  <span class=\"mf\">1.</span><span class=\"p\">]])</span>\n<span class=\"n\">Coordinates</span><span class=\"p\">:</span>\n  <span class=\"o\">*</span> <span class=\"n\">x</span>        <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"n\">int64</span> <span class=\"mi\">0</span> <span class=\"mi\">1</span>\n<span class=\"n\">Dimensions</span> <span class=\"n\">without</span> <span class=\"n\">coordinates</span><span class=\"p\">:</span> <span class=\"n\">y</span>\n</code></pre></div>\n<p>I hope this helps explain why you are getting some <code>NaNs</code> when using <code>drop=True</code>...</p>",
        "id": 42750,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1631663858
    },
    {
        "content": "<p>If you don't mind collapsing your results to a 1D array + losing coordinate/dimension information, you can use NumPy's <code>where()</code> in conjunction with slicing: </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">79</span><span class=\"p\">]:</span> <span class=\"n\">slice_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">array</span><span class=\"o\">.</span><span class=\"n\">data</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">80</span><span class=\"p\">]:</span> <span class=\"n\">array</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">slice_mask</span><span class=\"p\">]</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">80</span><span class=\"p\">]:</span> <span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">,</span> <span class=\"mf\">1.</span><span class=\"p\">])</span>\n</code></pre></div>",
        "id": 42751,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1631664449
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span>  Okay, I see your point, however in this case it would be okay to drop the nan locations because they should be the same for every time entry because they correspond to a time-invariant spatial mask. </p>\n<p>Consider the image below - it's sea ice concentration in a very specific region. We want to train a self organizing map (SOM) on these values.  In the figure all the purple points, which are the nans, will remain purple points for the full 100+ years of data we are looking at. The purple points correspond to areas we don't want to consider for training. The SOM uses a 1D vector array for each timestep we provide, and we provide many timesteps to train with.  Thus, we've flattened (or stacked) the xy data so that there's just a single array of values corresponding to the yellow and purple areas. <br>\n<a href=\"/user_uploads/2/b9/Mr2FwJHtYVvRb68gYuWtMHQI/image.png\">image.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/b9/Mr2FwJHtYVvRb68gYuWtMHQI/image.png\" title=\"image.png\"><img src=\"/user_uploads/2/b9/Mr2FwJHtYVvRb68gYuWtMHQI/image.png\"></a></div><p>However the SOM doesn't really need the time invariant points to train, so being able to reduce the size of the array passed to the SOM would be optimal. Thus, the desire to just drop the NaN points rather than keeping track of them. From that quick figure above you can see that probably 1/3-1/2 the domain is nans, so that's a lot of nans we want to drop to make the SOM process more efficient. Also, the SOM algorithm doesn't accept a \"NaN\" value, so at worst we could set the NaNs to something non-physical like -999 to be able to identify them but not break the algorithm.</p>\n<p>The equivalent might be like the following array:<br>\n([[1.0,  0.8, 1.0, nan, nan, 0.5, nan, 1.0],<br>\n   [0.9, 0.8, 0.9, nan, nan, 0.6, nan, 1.0],<br>\n   [0.9, 0.9, 0.8, nan, nan, 0.5, nan, 0.9],<br>\n   ..... (many more times here)])<br>\nimagine this array.shape = [8, 10,000]</p>\n<p>For each training time the nan stays in the same location for the vector. Thus, it would be more efficient to pass the following data to the SOM to train the following ideal array. In this case for each of the 10,000 time vectors there are only 5 values to train on rather than 8.<br>\n([[1.0,  0.8, 1.0, 0.5, 1.0],<br>\n   [0.9, 0.8, 0.9, 0.6, 1.0],<br>\n   [0.9, 0.9, 0.8, 0.5, 0.9],<br>\n   ..... (many more times here)])<br>\nimagine this array.shape = [5, 10,000]</p>\n<p>Does this make sense what I'm trying to do better and why I just want to drop those values? </p>\n<p>I see the comment that maybe the np.where could be used. However, I do <strong>not</strong> want to end up with the following by losing dimensionality:<br>\n[1.0,  0.8, 1.0, 0.5, 1.0, 0.9, 0.8, 0.9, 0.6, 1.0, 0.9, 0.9, 0.8, 0.5, 0.9, ....]<br>\nI could maybe loop through all the times and apply it to each time individually, but that seems inefficient. But maybe that's the way to do it? As I said, <span class=\"user-mention\" data-user-id=\"176\">@Maria Molina</span> and I have been trying to get around this and are stumped.</p>",
        "id": 42757,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1631675658
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span> Anderson's point is that there's a data model problem</p>\n<p>This example array (which I think looks like your end goal)</p>\n<div class=\"codehilite\"><pre><span></span><code>[[1, 2],\n [1, 2, 3]]\n</code></pre></div>\n<p>is not a regular array. It is \"ragged\" or \"awkward\" since the rows are of different lengths. These arrays cannot be represented by numpy arrays so xarray cannot represent them either. I think your choices are to flatten again to a 1D array then drop NaNs; or use a sentinel value like <code>-999</code> and preserve the 2 dimensions.</p>",
        "id": 42758,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1631677871
    },
    {
        "content": "<p>oh <span aria-label=\"face palm\" class=\"emoji emoji-1f926\" role=\"img\" title=\"face palm\">:face_palm:</span>  this figure is misleading. Ignore my last comment</p>\n<p>I think you want to experiment with</p>\n<div class=\"codehilite\"><pre><span></span><code>mask_stacked = mask.stack(horizontal=(&quot;nj&quot;, &quot;ni&quot;))\ndata.stack(horizontal=(&quot;nj&quot;, &quot;ni&quot;)).where(mask_stacked, drop=True)\n</code></pre></div>\n<p>basically stack the data and mask the same way to flatten the horizontal dimension; then drop.</p>",
        "id": 42759,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1631678694
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> Yes! It worked to stack the mask and the data and then drop that way rather than try to use the 2d mask with the data. Thanks so much! I know this took a lot of time for a lot of people. :)</p>",
        "id": 42835,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1631727904
    },
    {
        "content": "<p>Nice! Would you mind writing a very short summary of the lessons learned from this debugging discussion? I think we could make that a nice FAQ entry.</p>",
        "id": 42840,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1631731633
    },
    {
        "content": "<p>Yes, sure. I was out at the end of last week so just saw this, but I can work on it. :)</p>",
        "id": 43295,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1632246584
    },
    {
        "content": "<p>I'm trying to load some CESM2-LE members using the same intake-esm process I've used in the past. When I get to the point of doing ds.load() on the actual data I get this error: <br>\ndistributed.scheduler - ERROR - Couldn't gather keys {\"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 12, 18, 0, 0)\": ['tcp://10.12.206.32:33999'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 20, 6, 0, 0)\": ['tcp://10.12.206.35:40111'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 36, 1, 0, 0)\": ['tcp://10.12.206.35:40111'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 47, 10, 0, 0)\": ['tcp://10.12.206.37:40422'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 14, 14, 0, 0)\": ['tcp://10.12.206.56:44976'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 36, 10, 0, 0)\": ['tcp://10.12.206.37:40422'], ... (lots more)</p>\n<p>Any idea if this is a dask issue, a casper issue, or something else that might cause this error?</p>",
        "id": 44107,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1633101160
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/44107\">said</a>:</p>\n<blockquote>\n<p>I'm trying to load some CESM2-LE members using the same intake-esm process I've used in the past. When I get to the point of doing ds.load() on the actual data I get this error: <br>\ndistributed.scheduler - ERROR - Couldn't gather keys {\"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 12, 18, 0, 0)\": ['tcp://10.12.206.32:33999'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 20, 6, 0, 0)\": ['tcp://10.12.206.35:40111'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 36, 1, 0, 0)\": ['tcp://10.12.206.35:40111'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 47, 10, 0, 0)\": ['tcp://10.12.206.37:40422'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 14, 14, 0, 0)\": ['tcp://10.12.206.56:44976'], \"('concatenate-04c2be37fdc22c875ee22a7ba14c10e9', 36, 10, 0, 0)\": ['tcp://10.12.206.37:40422'], ... (lots more)</p>\n<p>Any idea if this is a dask issue, a casper issue, or something else that might cause this error?</p>\n</blockquote>\n<p>It looks like it's related to killed dask workers, so I'll follow on that and see if I can sort it out.</p>",
        "id": 44182,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1633117393
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/44182\">said</a>:</p>\n<blockquote>\n<p>It looks like it's related to killed dask workers, so I'll follow on that and see if I can sort it out.</p>\n</blockquote>\n<p>And just to round this out, I was able to get around the problem by using a .persist() instead of a .load() where I had been trying to load the data, and it seems to have worked fine and got past the problem. So issue has been solved (for now)!</p>",
        "id": 44189,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1633119860
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"134\">@Max Grover</span> I have been working with more CESM2-LE data and am a bit confused about how DASK breaks up chunks. I am loading some 3D, 5-day average data using intake-esm. Unsurprisingly the 3D fields and high temporal frequency mean I'm dealing with a lot of data here.</p>\n<p>Initially I had been trying to load all possible data and it looked like the chunk size was relatively reasonable (140MB): <a href=\"/user_uploads/2/1a/E8LFMjjhKUgi-uWPkQ_oG6va/Screen-Shot-2021-10-06-at-1.07.02-PM.png\">Screen-Shot-2021-10-06-at-1.07.02-PM.png</a><br>\nHowever, when I actually try to load the data I get the killed worker error message.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/1a/E8LFMjjhKUgi-uWPkQ_oG6va/Screen-Shot-2021-10-06-at-1.07.02-PM.png\" title=\"Screen-Shot-2021-10-06-at-1.07.02-PM.png\"><img src=\"/user_uploads/2/1a/E8LFMjjhKUgi-uWPkQ_oG6va/Screen-Shot-2021-10-06-at-1.07.02-PM.png\"></a></div><p>So I tried reducing the files I loaded in using intake esm to just use start_time for the period I'm interested in. I thought this might reduce the chunk size since I really only want 10 years, not the full 2015-2100 record. However, this has resulted in an even larger chunk size (6GB) with many fewer chunks. <a href=\"/user_uploads/2/9/ohA3NakTRlizcty1fdSKCaX9/Screen-Shot-2021-10-06-at-1.06.26-PM.png\">Screen-Shot-2021-10-06-at-1.06.26-PM.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/9/ohA3NakTRlizcty1fdSKCaX9/Screen-Shot-2021-10-06-at-1.06.26-PM.png\" title=\"Screen-Shot-2021-10-06-at-1.06.26-PM.png\"><img src=\"/user_uploads/2/9/ohA3NakTRlizcty1fdSKCaX9/Screen-Shot-2021-10-06-at-1.06.26-PM.png\"></a></div><p>I guess my question is how to get around this. I thought using intake-esm to load a minimum number of files would be helpful, but since it wasn't maybe I need to manually adjust chunk size and/or number of chunks? I've also tried using .load() or .persist() in different spots in the code, but the result is still the killed worker situation. Advice would be welcome.</p>",
        "id": 44479,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1633547585
    },
    {
        "content": "<p>Did you specify the chunksize when you read in the dataset? For example, if you want 5 times each chunk, you could specify this using the following:</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">dsets</span> <span class=\"o\">=</span> <span class=\"n\">catalog</span><span class=\"o\">.</span><span class=\"n\">to_dataset_dict</span><span class=\"p\">(</span><span class=\"n\">cdf_kwargs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">'chunks'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s1\">'time'</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">}})</span>\n</code></pre></div>\n<p>The <code>chunks</code> dictionary is used in <code>xarray</code> when using <code>open_mfdataset</code>. I would manually set this to make sure <code>xarray</code> does not try to assume anything...</p>",
        "id": 44480,
        "sender_full_name": "Max Grover",
        "timestamp": 1633547989
    },
    {
        "content": "<p>That was successful. Thanks, I'd forgotten about that little switch you can take. :)</p>",
        "id": 44481,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1633550416
    },
    {
        "content": "<p>I am having more issues with Dask and Killed Workers error messages. I am trying to load CESM2-LE 3D U and V data from CAM to calculate wind speed. It appears that the data load fails on the np.sqrt step when I look at the Dask Status  page to track tasks, but I'm not sure that's really where the problem is.</p>\n<p>I have tried reducing the chunksize (as Max had suggested back in October when I had similar issues) and that has not worked. I am at a bit of a loss of what to try next. <span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> <span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> , it seems like you're both Dask whisperers, any thoughts?</p>\n<p>PS. I seem to be the only one here with Dask issues - hah!</p>",
        "id": 52136,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647372097
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span>, Can you point me to the notebook you are using? I'm happy to look into this</p>",
        "id": 52139,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647375746
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> Yes, thanks! <br>\n| /glade/p/cgd/ppc/duvivier/cesm2_arctic_cyclones/rufmod_analysis/version_3/wind_response/wind_trends_vertical.ipynb</p>\n<p>Partway down you'll see my note <strong>STOP</strong> right after I try loading things because that's where I was getting stuck. Please let me know if you have other questions. I really appreciate the help. :)</p>",
        "id": 52146,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647377802
    },
    {
        "content": "<p>Thank you, <span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span>! I am unable to run the notebook because  It appears that files under <code>/glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/</code> are not publicly readable</p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>$ ls -ltrh /glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/\nls: cannot access /glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/: Permission denied\n</code></pre></div>",
        "id": 52151,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647379860
    },
    {
        "content": "<p>I just changed the permissions. Sorry about that. I thought they were changed before, but I guess not.</p>\n<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/52151\">said</a>:</p>\n<blockquote>\n<p>Thank you, <span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span>! I am unable to run the notebook because  It appears that files under <code>/glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/</code> are not publicly readable</p>\n<p><div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>$ ls -ltrh /glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/\nls: cannot access /glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/: Permission denied\n</code></pre></div><br>\n</p>\n</blockquote>",
        "id": 52152,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647380500
    },
    {
        "content": "<p>Something may have gone wrong during the permission update.... I'm getting </p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code> ls -ltrh /glade/campaign/cesm/development/pcwg/duvivier/\nls: cannot access /glade/campaign/cesm/development/pcwg/duvivier/mosaic: Permission denied\nls: cannot access /glade/campaign/cesm/development/pcwg/duvivier/VarRes_Greenland: Permission denied\nls: cannot access /glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones: Permission denied\ntotal <span class=\"m\">0</span>\nd????????? ? ? ? ?            ? VarRes_Greenland\nd????????? ? ? ? ?            ? mosaic\nd????????? ? ? ? ?            ? arctic_cyclones\n</code></pre></div>",
        "id": 52153,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647380626
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> Hum, when I look at all the files and directories they show they're readable and executable for everyone.</p>\n<p>Are you able to ls this file:<br>\n|/glade/campaign/cesm/development/pcwg/duvivier/arctic_cyclones/b.e21.BSSP370.f09_g17.rufmod.001/atm/proc/tseries/month_1/b.e21.BSSP370.f09_g17.rufmod.001.cam.h0.U.201501-206412.nc</p>",
        "id": 52154,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647380986
    },
    {
        "content": "<p>It's working now</p>",
        "id": 52155,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647381767
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"153\">@Alice DuVivier</span>, I was able to run the notebook successfully up to the cell marked <code>STOP</code>. I don't know why you are getting the <code>distributed.scheduler - ERROR - Workers don't have promised key</code> errors... When you get a chance, can you post here the output of <code>dask.config.get('jobqueue.pbs.log-directory')</code>? We may be able to find some answers in the dask workers' logs</p>",
        "id": 52156,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647382488
    },
    {
        "content": "<p>Hum... I'm on PBS Casper with 20GB memory.  When I look at the DASK dashboard it just shows that all processes are complete except one for the sqrt (blue) status bar at the bottom and it just hangs there for a long time.  <a href=\"/user_uploads/2/6/94s2CpKiajjSJ8DzFwn0t6da/Screen-Shot-2022-03-15-at-4.41.44-PM.png\">Screen-Shot-2022-03-15-at-4.41.44-PM.png</a> </p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/2/6/94s2CpKiajjSJ8DzFwn0t6da/Screen-Shot-2022-03-15-at-4.41.44-PM.png\" title=\"Screen-Shot-2022-03-15-at-4.41.44-PM.png\"><img src=\"/user_uploads/2/6/94s2CpKiajjSJ8DzFwn0t6da/Screen-Shot-2022-03-15-at-4.41.44-PM.png\"></a></div><p>Then the error that comes up in my notebook now is the following. <br>\nCommClosedError: in &lt;TCP (closed) ConnectionPool.gather local=tcp://10.12.206.51:45898 remote=tcp://10.12.206.51:35757&gt;: ConnectionResetError: [Errno 104] Connection reset by peer</p>\n<p>I've been trying various things here and earlier I had a longer message that had the following types of examples:<br>\n<code>distributed.scheduler - ERROR - Couldn't gather keys {\"('getitem-70c66b0a352ae06959a0335384d383c8', 24, 2, 0, 0, 0)\": ['tcp://10.12.206.59:41792'], \"('getitem-70c66b0a352ae06959a0335384d383c8', 32, 5, 0, 0, 0)\": ['tcp://10.12.206.59:45689'], \"('getitem-70c66b0a352ae06959a0335384d383c8', 26, 7, 0, 0, 0)\": </code></p>\n<p><code>NoneType: None\ndistributed.scheduler - ERROR - Workers don't have promised key: ['tcp://10.12.206.59:41792'], ('getitem-70c66b0a352ae06959a0335384d383c8', 24, 2, 0, 0, 0)</code></p>\n<p>Killed Worker...</p>",
        "id": 52164,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647384650
    },
    {
        "content": "<p>I'm not sure I know how to get the info from the <code>dask.config.get('jobqueue.pbs.log-directory')</code> command.</p>",
        "id": 52165,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647384760
    },
    {
        "content": "<p>Follow up. I poked more through the dask tabs and maybe the workers weren't all killed now that I reduced the chunksize? The error before that reduction was the 'Killed Worker' one. That being said, I'm not sure what the <code>CommClosedError</code> message is about and if it's a show stopper.</p>",
        "id": 52166,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647385585
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/52165\">said</a>:</p>\n<blockquote>\n<p>I'm not sure I know how to get the info from the <code>dask.config.get('jobqueue.pbs.log-directory')</code> command.</p>\n</blockquote>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">dask</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'jobqueue.pbs.log-directory'</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>should tell us where on <code>/glade</code> Dask is persisting worker logs...</p>",
        "id": 52167,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647385772
    },
    {
        "content": "<p>Got it! Thanks.<br>\n/glade/scratch/duvivier/dask/casper-dav/logs</p>",
        "id": 52168,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647386011
    },
    {
        "content": "<p>Thanks...</p>",
        "id": 52169,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647386076
    },
    {
        "content": "<p>I'm sorry this is such a PITA</p>",
        "id": 52170,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647386092
    },
    {
        "content": "<p>No problem at all... When was your latest attempt/run? It appears that some of your workers are being killed by PBS </p>\n<p>Here's one:</p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>$ cat /glade/scratch/duvivier/dask/casper-dav/logs/2290461.casper-pbs.ER\n</code></pre></div>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>distributed.worker - INFO - Waiting to connect to:   tcp://10.12.206.18:33959\n<span class=\"o\">=</span>&gt;&gt; PBS: job killed: walltime <span class=\"m\">14401</span> exceeded limit <span class=\"m\">14400</span>\ndistributed.dask_worker - INFO - Exiting on signal <span class=\"m\">15</span>\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">'tcp://10.12.206.32:42650'</span>\n/glade/work/duvivier/miniconda3/envs/analysis2/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be <span class=\"m\">1</span> leaked semaphores to clean up at shutdown\n  len<span class=\"o\">(</span>cache<span class=\"o\">))</span>\ndistributed.nanny - INFO - Worker process <span class=\"m\">162031</span> was killed by signal <span class=\"m\">15</span>\ndistributed.dask_worker - INFO - End worker\n</code></pre></div>",
        "id": 52172,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647386211
    },
    {
        "content": "<p>This may (partially) explain the <code>CommClosedError</code></p>",
        "id": 52173,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647386280
    },
    {
        "content": "<p>I just tried within the last 30 min. That's when those last error logs will be from. And I think my latest PBS session request would end around 6:30. So I don't <em>think</em> it should be shutting down yet.</p>",
        "id": 52174,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647386289
    },
    {
        "content": "<p>I wonder if I should just start a new session or something? And maybe more memory? Does that seem like a reasonable first step?</p>",
        "id": 52175,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647386311
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/52175\">said</a>:</p>\n<blockquote>\n<p>I wonder if I should just start a new session or something? And maybe more memory? Does that seem like a reasonable first step?</p>\n</blockquote>\n<p>That might help... I should point I used the following configuration when I last ran the notebook successfully</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"n\">processes</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n<span class=\"n\">cores</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n<span class=\"n\">memory</span> <span class=\"o\">=</span> <span class=\"s1\">'100GB'</span>\n\n<span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">PBSCluster</span><span class=\"p\">(</span><span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"n\">cores</span><span class=\"p\">,</span> <span class=\"n\">processes</span><span class=\"o\">=</span><span class=\"n\">processes</span><span class=\"p\">,</span>\n                     <span class=\"n\">resource_spec</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s1\">'select=1:ncpus=</span><span class=\"si\">{</span><span class=\"n\">processes</span><span class=\"si\">}</span><span class=\"s1\">:mem=</span><span class=\"si\">{</span><span class=\"n\">memory</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">,</span>\n                     <span class=\"n\">walltime</span><span class=\"o\">=</span><span class=\"s1\">'01:00:00'</span><span class=\"p\">,</span> <span class=\"n\">interface</span><span class=\"o\">=</span><span class=\"s1\">'ib0'</span><span class=\"p\">)</span>\n\n<span class=\"n\">cluster</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n</code></pre></div>",
        "id": 52176,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647386763
    },
    {
        "content": "<p>Also, in this cell, did you mean to assign <code>member_id</code> to coords?</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"c1\"># set member_id values</span>\n<span class=\"n\">futures_1</span><span class=\"o\">.</span><span class=\"n\">member_id</span><span class=\"o\">.</span><span class=\"n\">values</span>\n<span class=\"n\">futures_2</span><span class=\"o\">.</span><span class=\"n\">member_id</span><span class=\"o\">.</span><span class=\"n\">values</span>\n\n<span class=\"c1\"># assign member_id as coordinate array</span>\n<span class=\"n\">futures_1</span><span class=\"o\">.</span><span class=\"n\">assign_coords</span><span class=\"p\">({</span><span class=\"s2\">\"member_id\"</span><span class=\"p\">:</span> <span class=\"n\">futures_1</span><span class=\"o\">.</span><span class=\"n\">member_id</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">})</span>\n<span class=\"n\">futures_2</span><span class=\"o\">.</span><span class=\"n\">assign_coords</span><span class=\"p\">({</span><span class=\"s2\">\"member_id\"</span><span class=\"p\">:</span> <span class=\"n\">futures_2</span><span class=\"o\">.</span><span class=\"n\">member_id</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">})</span>\n</code></pre></div>\n<p>if so, the last two lines should be </p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"c1\"># assign member_id as coordinate array</span>\n<span class=\"n\">futures_1</span> <span class=\"o\">=</span> <span class=\"n\">futures_1</span><span class=\"o\">.</span><span class=\"n\">assign_coords</span><span class=\"p\">({</span><span class=\"s2\">\"member_id\"</span><span class=\"p\">:</span> <span class=\"n\">futures_1</span><span class=\"o\">.</span><span class=\"n\">member_id</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">})</span>\n<span class=\"n\">futures_2</span> <span class=\"o\">=</span> <span class=\"n\">futures_2</span><span class=\"o\">.</span><span class=\"n\">assign_coords</span><span class=\"p\">({</span><span class=\"s2\">\"member_id\"</span><span class=\"p\">:</span> <span class=\"n\">futures_2</span><span class=\"o\">.</span><span class=\"n\">member_id</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">})</span>\n</code></pre></div>",
        "id": 52177,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647386892
    },
    {
        "content": "<p>Otherwise, the <code>.assign_coords()</code> won't do what you what since it doesn't happen inplace</p>",
        "id": 52178,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647386937
    },
    {
        "content": "<p>I will try those options. I have to do kid-care things, but I'll follow up later tonight or early tomorrow. thanks so much!! :)</p>",
        "id": 52179,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647386990
    },
    {
        "content": "<p>You're welcome!... Keep me posted</p>",
        "id": 52180,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647387019
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/52167\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask/near/52165\">said</a>:</p>\n<blockquote>\n<p>I'm not sure I know how to get the info from the <code>dask.config.get('jobqueue.pbs.log-directory')</code> command.</p>\n</blockquote>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">dask</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">dask</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">'jobqueue.pbs.log-directory'</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>should tell us where on <code>/glade</code> Dask is persisting worker logs...</p>\n</blockquote>\n<p>This would be useful to put in the FAQ</p>",
        "id": 52234,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1647402262
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> It looks like it worked today with the configuration you suggested. Hooray! It took a long time (35min according to the time output), but it didn't die and I think I'm going to call that a win for now. Thanks!</p>",
        "id": 52239,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647445786
    },
    {
        "content": "<p>Glad to hear it! </p>\n<p><span class=\"user-mention silent\" data-user-id=\"153\">Alice DuVivier</span> <a href=\"#narrow/stream/41-ESDS/topic/Dask.3A.20KilledWorker.20Error/near/52164\">said</a>:</p>\n<blockquote>\n<p>Hum... I'm on PBS Casper with 20GB memory.  When I look at the DASK dashboard it just shows that all processes are complete except one for the sqrt (blue) status bar at the bottom and it just hangs there for a long time.  <a href=\"/user_uploads/2/6/94s2CpKiajjSJ8DzFwn0t6da/Screen-Shot-2022-03-15-at-4.41.44-PM.png\">Screen-Shot-2022-03-15-at-4.41.44-PM.png</a> </p>\n</blockquote>\n<p>It's still unclear to me why Dask was getting stuck at that one <code>sqrt</code> task</p>",
        "id": 52262,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647448647
    },
    {
        "content": "<blockquote>\n<p>Hooray! It took a long time (35min according to the time output)</p>\n</blockquote>\n<p>Not sure if this has to do with your cluster size. Were all your workers available or did you have some jobs pending in the queue?</p>",
        "id": 52263,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1647448716
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> things were running really slowly after this step with the vertical interpolation yesterday. I didn't finish up that analysis and code and I'm going to be out of the office for a few days. But I'll check back in when I'm back!</p>",
        "id": 52307,
        "sender_full_name": "Alice DuVivier",
        "timestamp": 1647522798
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"41\" href=\"/#narrow/stream/41-ESDS/topic/Dask.3A.20KilledWorker.20Error\">#ESDS &gt; Dask: KilledWorker Error</a> by <span class=\"user-mention silent\" data-user-id=\"13\">Anderson Banihirwe</span></p>",
        "id": 52308,
        "sender_full_name": "Notification Bot",
        "timestamp": 1647523438
    }
]