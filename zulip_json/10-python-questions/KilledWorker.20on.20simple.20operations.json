[
    {
        "content": "<p>Hi all. Generally <code>dask</code> plays pretty nicely with my work if I throw a lot of workers at the problem. I'm struggling with a <code>KilledWorker</code> error right now in a pretty simple case. I have a 32.4GB dataset and I am taking a mean over a single dimension to reduce it to ~6.5GB dataset. I tried with 36 cores per node, 36 processes, 100GB memory, and 10 nodes (i.e. 1TB memory and 360 workers). It's chunked such that chunks are about 100MB. Just trying to take the mean over a single dimension and then saving to a netcdf I get </p>\n<div class=\"codehilite\"><pre><span></span><span class=\"n\">KilledWorker</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"s2\">&quot;(&#39;open_dataset-concatenate-c100c09070937c41c0bc3dec0d93593c&#39;, 0, 5, 0, 0, 0)&quot;</span><span class=\"p\">,</span> <span class=\"o\">&lt;</span><span class=\"n\">Worker</span> <span class=\"s1\">&#39;tcp://10.148.7.66:46303&#39;</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"o\">-</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">memory</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">processing</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"o\">&gt;</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>Any thoughts? I tried 18 processes per node instead of 36 to give the workers a little more memory. I've also tried to chunk smaller and larger. Again, generally more workers/nodes solves these kinds of things but these are 100MB chunks with 360 workers for a 32GB dataset. Shouldn't expect this problem.</p>",
        "id": 10845,
        "sender_full_name": "Riley Brady",
        "timestamp": 1591726840
    },
    {
        "content": "<p>does <code>ds.mean(\"dim\").compute()</code> also crash?</p>",
        "id": 10846,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1591727077
    },
    {
        "content": "<p>and is the 32.4GB dataset being read straight from disk, or is the result of some other step?</p>",
        "id": 10847,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1591727125
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> , yep it does. I switched over to <code>.to_netcdf()</code> thinking that might help for some reason. It's read semi straight from disk. I run <code>xr.open_mfdataset</code> first to concatenate 7 ~4.5GB files into the larger one. Maybe that's what the concat error is. I might try doing the <code>open_mfdataset</code> and then saving the combined form out as zarr.</p>",
        "id": 10848,
        "sender_full_name": "Riley Brady",
        "timestamp": 1591727208
    },
    {
        "content": "<p>are you chunking at the <code>mfdataset</code> stage?</p>",
        "id": 10849,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1591727257
    },
    {
        "content": "<p>I think that fixes it. :) I was running <code>xr.open_mfdataset(files, parallel=True)</code> then chunking afterwards with <code>ds = ds.chunk(...)</code>. I didn't stop to think that it was probably loading it all in as one chunk and then rechunking. It works if I run <code>chunks=...</code> within the <code>open_mfdataset</code> stage. Thanks! Sometimes it's simple.</p>",
        "id": 10850,
        "sender_full_name": "Riley Brady",
        "timestamp": 1591727513
    }
]