[
    {
        "content": "<p>Hi all,</p>\n<p>I've previously used <a href=\"https://ncar.github.io/esds/posts/2022/sparse-PFT-gridding/#wrapping-sparse-arrays-in-xarray\">this nifty bit of code</a> by <span class=\"user-mention\" data-user-id=\"25\">@Deepak Cherian</span> and <span class=\"user-mention\" data-user-id=\"47\">@Katie Dagon</span> to process 1D output from CLM5 (i.e. carbon variables at PFT level). I last did this a couple of years ago and am now having problems when I return to it. I can read in an input file and process it into an xarray with no issues. However, attempting to do fairly basic processing immediately kills my Jupyter notebook (which has 16GB of memory). This happens if, for example, I attempt to take the global sum of some variable and then either plot the output as a timeseries or dump the values as a numpy array.</p>\n<p>Illustrative example below, which takes as input a file containing PFT-level CLM5 output on a 1deg global grid covering 2025-2100 and using the functions defined in the link:</p>\n<div class=\"codehilite\"><pre><span></span><code>ifile = xr.open_dataset(file.nc)\npft_constants = xr.open_dataset(clm5_params_file.nc)\npftnames = pft_constants.pftname\n\nsparse_data = convert_pft_variables_to_sparse(ifile, pftnames)\nout = sparse_data.FireEmis_TOT.isel(vegtype=1).sum((&quot;lon&quot;, &quot;lat&quot;))\nprint(out.values)\n</code></pre></div>\n<p>At this point, the notebook crashes. Should I expect large memory requirements for this?</p>",
        "id": 105229,
        "sender_full_name": "James King",
        "timestamp": 1735916963
    },
    {
        "content": "<p>I wonder if some of the operations you're doing are converting large arrays to non-sparse array types.  Have you tested out the analysis operations you're interested in on a smaller subset to see if the resulting arrays are still sparse array types?  You should be able to see the array type in the description of the Xarray DataArray in your notebook.</p>",
        "id": 105230,
        "sender_full_name": "Katelyn FitzGerald",
        "timestamp": 1735929724
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"302\">@James King</span> I've also noticed that the pft regridding is a bit of a memory hog (and I'm only looking at a decade of data, not 75 years).  I can't remember if the script is friendly with dask, but I'd recommend throwing more memory at the problem, either for asking for more when you log onto jupyter hub or by using dask workers?</p>",
        "id": 105241,
        "sender_full_name": "Will Wieder",
        "timestamp": 1736273867
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"339\">@Katelyn FitzGerald</span> and <span class=\"user-mention\" data-user-id=\"67\">@Will Wieder</span>  - alas I'm working on a non-NCAR system which doesn't allow me to request any more memory than that for a Jupyter notebook! Testing on a smaller subset of data sounds wise, this is probably a case of me being impatient to produce some nice plots...</p>",
        "id": 105260,
        "sender_full_name": "James King",
        "timestamp": 1736502645
    },
    {
        "content": "<p>And yes, the script makes use of dask</p>",
        "id": 105261,
        "sender_full_name": "James King",
        "timestamp": 1736521426
    }
]