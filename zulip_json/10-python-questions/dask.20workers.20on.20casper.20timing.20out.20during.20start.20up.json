[
    {
        "content": "<p>I'm trying to do some dask-enabled computations on casper/dav, which I'm using because the input files are on campaign storage. I recently realized that my scripts have been consistently slower than they used to be. After digging in, I realized that the computations are essentially serial, because the dask workers are being canceled shortly after they start up. The slurm job standard error files for the workers all contain</p>\n<div class=\"codehilite\"><pre><span></span>distributed.dask_worker - INFO - Timed out starting worker\ndistributed.dask_worker - INFO - End worker\n</pre></div>\n\n\n<p>I'm trying to figure out if this is being caused by a change in the system or from change in my environment and/or scripts. I'm not sure how long this has been going on. Have others seen this behavior on casper?</p>",
        "id": 867,
        "sender_full_name": "Keith Lindsay",
        "timestamp": 1579962285
    },
    {
        "content": "<blockquote>\n<p>Have others seen this behavior on casper?</p>\n</blockquote>\n<p>I haven't encountered this behavior myself, but I've seen similar reports online before. Can you point me to the location of  the log file?</p>",
        "id": 871,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1579976295
    },
    {
        "content": "<blockquote>\n<p>I haven't encountered this behavior myself</p>\n</blockquote>\n<p>Scratch that <span aria-label=\"slight smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"slight smile\">:slight_smile:</span>  I just started seeing this behavior myself. I don't know what's going on yet:</p>\n<div class=\"codehilite\"><pre><span></span>distributed.worker - INFO -       Start worker at:   tcp://10.12.205.25:36228\ndistributed.worker - INFO -          Listening to:   tcp://10.12.205.25:36228\ndistributed.worker - INFO -          dashboard at:         <span class=\"m\">10</span>.12.205.25:40625\ndistributed.worker - INFO - Waiting to connect to: tcp://128.117.181.206:36764\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          <span class=\"m\">1</span>\ndistributed.worker - INFO -                Memory:                   <span class=\"m\">50</span>.00 GB\ndistributed.worker - INFO -       Local Directory: /glade/scratch/abanihi/worker-nww69z_s\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.nanny - INFO - Closing Nanny at <span class=\"s1\">&#39;tcp://10.12.205.25:44881&#39;</span>\ndistributed.worker - INFO - Stopping worker at tcp://10.12.205.25:36228\ndistributed.worker - INFO - Closed worker has not yet started: None\ndistributed.dask_worker - INFO - Timed out starting worker\ndistributed.dask_worker - INFO - End worker\n</pre></div>",
        "id": 872,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1579981142
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"31\">@Keith Lindsay</span></p>",
        "id": 879,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1579995966
    },
    {
        "content": "<p>Were you using <code>cluster.adapt()</code> or <code>cluster.scale()</code> ??</p>",
        "id": 880,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1579995985
    },
    {
        "content": "<p><code>cluster.scale()</code><br>\nThe client is being instantiated in function <code>_tseries_gen</code> of <code>/glade/work/klindsay/analysis/CESM2_coup_carb_cycle_JAMES/src/tseries_mod.py</code></p>",
        "id": 881,
        "sender_full_name": "Keith Lindsay",
        "timestamp": 1579996150
    },
    {
        "content": "<p>I used cluster.scale()</p>",
        "id": 882,
        "sender_full_name": "Precious Mongwe",
        "timestamp": 1579996161
    },
    {
        "content": "<p>Sorry, \"this is a different conversation\"</p>",
        "id": 883,
        "sender_full_name": "Precious Mongwe",
        "timestamp": 1579996211
    },
    {
        "content": "<blockquote>\n<p><code>cluster.scale()</code><br>\nThe client is being instantiated in function <code>_tseries_gen</code> of <code>/glade/work/klindsay/analysis/CESM2_coup_carb_cycle_JAMES/src/tseries_mod.py</code></p>\n</blockquote>\n<p>After some digging and looking at <code>tseries_mod.py</code>, <strong>my speculation so far  is that this happens just when a worker comes online</strong>. The error is logged by <a href=\"https://github.com/dask/distributed/blob/4081a0e09febd9099f04a1db9772f6cd95203b2e/distributed/cli/dask_worker.py#L426\" target=\"_blank\" title=\"https://github.com/dask/distributed/blob/4081a0e09febd9099f04a1db9772f6cd95203b2e/distributed/cli/dask_worker.py#L426\">this line</a>. Throughout <code>tseries_mod.py</code>, the cluster is scaled up and down in a few places, and it's likely that in some cases the following is happening:</p>\n<ul>\n<li><code>cluster.scale_up(num_workers)</code> is called, </li>\n<li>xarray and dask go on to do the computation with number of workers less than the requested<code>num_workers</code>.  This leaves a few workers in the queue ( Slurm or PBS hasn't provisioned the resources for some of the workers yet)</li>\n<li>The computation finishes, and then <code>cluster.scale_down(num_workers)</code> is called. A <code>signal</code> is then sent to some workers to shut down. The workers which are still in the PBS/SLURM queue receive the shut down signal before  <em>getting a chance to start</em>, which could explain the logs:</li>\n</ul>\n<div class=\"codehilite\"><pre><span></span>distributed.nanny - INFO - Closing Nanny at <span class=\"s1\">&#39;tcp://10.12.205.25:44881&#39;</span>\ndistributed.worker - INFO - Stopping worker at tcp://10.12.205.25:36228\ndistributed.worker - INFO - Closed worker has not yet started: None\ndistributed.dask_worker - INFO - Timed out starting worker\ndistributed.dask_worker - INFO - End worker\n</pre></div>\n\n\n<p><code>scaling</code> the cluster once (i.e. calling <code>cluster.scale()</code> once) throughout the computation may provide more insights.</p>\n<p>I will do more digging, and will let you know if I find anything</p>",
        "id": 884,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1579998324
    },
    {
        "content": "<p>Thanks for taking a look <span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> </p>\n<p>While <code>tseries_mod.py</code> can end up calling <code>cluster.scale()</code> multiple times with different arguments, the problems that I'm currently encountering are occurring with <code>cluster.scale()</code> having been called just once. I just added a <code>print</code> statement to <code>tseries_mod.py</code> to confirm this and reran one of my notebooks. Now I'm getting an error from <code>distributed</code> that I don't recall encountering previously:</p>\n<div class=\"codehilite\"><pre><span></span>distributed.utils - ERROR - addresses should be strings or tuples, got None\nTraceback (most recent call last):\n  File \"/glade/work/klindsay/miniconda3/envs/CESM2_coup_carb_cycle_JAMES/lib/python3.7/site-packages/distributed/utils.py\", line 662, in log_errors\n    yield\n  File \"/glade/work/klindsay/miniconda3/envs/CESM2_coup_carb_cycle_JAMES/lib/python3.7/site-packages/distributed/scheduler.py\", line 2122, in remove_worker\n    address = self.coerce_address(address)\n  File \"/glade/work/klindsay/miniconda3/envs/CESM2_coup_carb_cycle_JAMES/lib/python3.7/site-packages/distributed/scheduler.py\", line 4831, in coerce_address\n    raise TypeError(\"addresses should be strings or tuples, got %r\" % (addr,))\nTypeError: addresses should be strings or tuples, got None\n</pre></div>\n<p>I see some github issues on this <a href=\"https://github.com/dask/distributed/issues/3374\" target=\"_blank\" title=\"https://github.com/dask/distributed/issues/3374\">GH #3374</a> and <a href=\"https://github.com/dask/distributed/issues/3386\" target=\"_blank\" title=\"https://github.com/dask/distributed/issues/3386\">GH #3386</a> that I'll take a look at.<br/>\nPerhaps I got some incompatibilities in a recent environment update.</p>",
        "id": 885,
        "sender_full_name": "Keith Lindsay",
        "timestamp": 1580001870
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"13\">@Anderson Banihirwe</span> , I tried using various earlier versions of distributed, which also led to me using earlier versions of dask. These attempts didn't pan out.</p>\n<p>However, I realized that my script was working on some datasets, the workers didn't fail for all datasets. The datasets that are leading to failed workers are 4D (3D space+time), while the ones that work are 3D (2D space+time). For 4D datasets, I'm using smaller chunk sizes in time, in an attempt to keep memory per chunk from getting too big. I tried using small time chunks for 3D datasets and this led to failed workers, telling me that I was on to something. So I tried increasing the time chunksize for 4D datasets. This worked, and I'm now up and running.</p>\n<p>I have to admit that I don't understand why the small time chunk sizes are a problem now, while they previously wasn't.</p>\n<p>I don't know if this is related at all to why you are seeing the symptom of failed workers.</p>",
        "id": 886,
        "sender_full_name": "Keith Lindsay",
        "timestamp": 1580015312
    },
    {
        "content": "<p>With the larger chunks, I did find it necessary to increase memory per worker in <code>~/.dask/jobqueue.yaml</code>.</p>",
        "id": 887,
        "sender_full_name": "Keith Lindsay",
        "timestamp": 1580016268
    }
]