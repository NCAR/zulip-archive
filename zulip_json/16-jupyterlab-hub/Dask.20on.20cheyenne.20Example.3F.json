[
    {
        "content": "<p>I'm unable to get the Dask dashboard working on cheyenne.   </p>\n<p>Is anyone able to post an example of importing and running Dask with the dashboard using JupyterHub on Cheyenne?   I would be very grateful, as Casper resources are very hard to get right now.   I am using Dask through a conda environment that I built on Casper, so I'm a little worried that it might not be configured properly to work on cheyenne.</p>",
        "id": 17112,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599688622
    },
    {
        "content": "<blockquote>\n<p>I'm unable to get the Dask dashboard working on cheyenne. </p>\n</blockquote>\n<p>Are you getting an <code>404</code> error or some other error?</p>",
        "id": 17113,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599689061
    },
    {
        "content": "<p>I don't know what the dashboard URL is supposed to be.   I'm not seeing an HTTP error (yet).</p>",
        "id": 17114,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599689142
    },
    {
        "content": "<blockquote>\n<p>I don't know what the dashboard URL is supposed to be.  </p>\n</blockquote>\n<p>The template of the dashboard link should be <code> \"https://jupyterhub.ucar.edu/ch/user/{USER}/proxy/{port}/status\"</code></p>",
        "id": 17116,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599689405
    },
    {
        "content": "<p>Are you using <code>ncar-jobqueue</code> or <code>dask-jobqueue</code> to instantiate the cluster?</p>",
        "id": 17117,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599689441
    },
    {
        "content": "<p>Thank you, I will try that.   Do you also happen to know what parameters to pass when creating the cluster, so that CPU and memory resources are reasonable for the cheyenne architecture?</p>",
        "id": 17118,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599689562
    },
    {
        "content": "<p>Try </p>\n<div class=\"codehilite\"><pre><span></span><span class=\"n\">cores</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n<span class=\"n\">processes</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">memory</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;109GB&#39;</span>\n</pre></div>",
        "id": 17120,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599689809
    },
    {
        "content": "<p>if you find yourself getting <code>KilledWorker</code> errors, try reducing the number of processes</p>",
        "id": 17121,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599689897
    },
    {
        "content": "<p>Thank you, I am assuming I have to use <code>ncar-jobqueue</code> on cheyenne.  I will post shortly the code I am trying.</p>",
        "id": 17122,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599690598
    },
    {
        "content": "<blockquote>\n<p>I am assuming I have to use ncar-jobqueue on cheyenne </p>\n</blockquote>\n<p>When using <code>ncar-jobqueue</code> in conjunction with the <code>hub</code>, the dashboard link is set for you i.e. should work out of the box without you needing to set it manually.</p>",
        "id": 17123,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599690752
    },
    {
        "content": "<p>I was worried that by using my own Dask configuration, that the dashboard link would not be set by default, but I will try it as you suggest.</p>",
        "id": 17124,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599690905
    },
    {
        "content": "<p>Does this code look reasonable?</p>\n<div class=\"codehilite\"><pre><span></span>import dask\n\nmachine = &#39;cheyenne&#39;  # &#39;casper&#39;\n\nif machine == &#39;cheyenne&#39;:\n    # The following is supposedly set when using NCARCluster\n    #dask.config.set({&#39;distributed.dashboard.link&#39;: &quot;https://jupyterhub.ucar.edu/ch/user/{USER}/proxy/{port}/status&quot;})\n    from ncar_jobqueue import NCARCluster\n    cluster = NCARCluster(cores=10, processes=20, memory=&#39;109GB&#39;, project=&#39;STDD0003&#39;)\n    cluster.scale(jobs=20)\nelse:\n    # Assume machine is Casper.\n    dask.config.set({&#39;distributed.dashboard.link&#39;: &#39;/proxy/{port}/status&#39;})\n    from dask_jobqueue import SLURMCluster\n    cluster = SLURMCluster(cores=8, memory=&#39;200GB&#39;, project=&#39;STDD0003&#39;)\n    cluster.scale(jobs=8)\n\nfrom distributed import Client\nclient = Client(cluster)\ncluster\n</pre></div>",
        "id": 17125,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599691160
    },
    {
        "content": "<p>The primary objective of <code>ncar-jobqueue</code> is to abstract the <code>if</code> statements...  </p>\n<p>My recommendation is to just use </p>\n<div class=\"codehilite\"><pre><span></span>from ncar_jobqueue import NCARCluster\ncluster = NCARCluster(cores=10, processes=20, memory=&#39;109GB&#39;, project=&#39;STDD0003&#39;)\n</pre></div>\n\n\n<p>If you want to have separate configurations (such as memory, cores, etcc) for casper and/or cheyenne, You could just put them in the <code>~/.config/dask/jobqueue.yaml</code> file</p>",
        "id": 17126,
        "sender_full_name": "Anderson Banihirwe",
        "timestamp": 1599691522
    },
    {
        "content": "<p>OK, that is good information, I will give it a try on both systems and report back if there are problems.  Thank you again!</p>",
        "id": 17127,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599692156
    },
    {
        "content": "<p>OK, the dashboard works on cheyenne now, but Dask freezes with errors.    I might be related to the operations I am trying, but still I wonder if I have the right package versions for Dask.</p>\n<p>I am using these package versions:</p>\n<div class=\"codehilite\"><pre><span></span>bokeh                     1.4.0            py38h32f6830_1    conda-forge\ndask                      2.14.0                     py_0    conda-forge\ndask-core                 2.14.0                     py_0    conda-forge\ndask-jobqueue             0.7.1                      py_0    conda-forge\ndistributed               2.14.0           py38h32f6830_0    conda-forge\nncar-jobqueue             2020.3.4                 pypi_0    pypi\n</pre></div>\n\n\n<div class=\"codehilite\"><pre><span></span>tornado.application - ERROR - Exception in callback &lt;bound method BokehTornado._keep_alive of &lt;bokeh.server.tornado.BokehTornado object at 0x2b3a03b7a790&gt;&gt;\nTraceback (most recent call last):\n  File &quot;/glade/u/home/bonnland/miniconda3/envs/lens-conversion/lib/python3.8/site-packages/tornado/ioloop.py&quot;, line 907, in _run\n    return self.callback()\n  File &quot;/glade/u/home/bonnland/miniconda3/envs/lens-conversion/lib/python3.8/site-packages/bokeh/server/tornado.py&quot;, line 579, in _keep_alive\n    c.send_ping()\n  File &quot;/glade/u/home/bonnland/miniconda3/envs/lens-conversion/lib/python3.8/site-packages/bokeh/server/connection.py&quot;, line 80, in send_ping\n    self._socket.ping(codecs.encode(str(self._ping_count), &quot;utf-8&quot;))\n  File &quot;/glade/u/home/bonnland/miniconda3/envs/lens-conversion/lib/python3.8/site-packages/tornado/websocket.py&quot;, line 447, in ping\n    raise WebSocketClosedError()\ntornado.websocket.WebSocketClosedError\n</pre></div>\n\n\n<p>I am also getting back warnings from<code>intake-esm.to_dataset_dict()</code> like this:</p>\n<div class=\"codehilite\"><pre><span></span>/glade/u/home/bonnland/miniconda3/envs/lens-conversion/lib/python3.8/site-packages/dask/array/core.py:3911: PerformanceWarning: Increasing number of chunks by factor of 65\n  result = blockwise(\n</pre></div>\n\n\n<p>I don't think I am specifying too few chunks...</p>",
        "id": 17146,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599693955
    },
    {
        "content": "<p>UPDATE:  I was specifying too few chunks after all, when trying to create a Zarr store.   After increasing the number of chunks and being patient, I was able to get past the original errors.   The error messages still appear, but do not cause the overall computation to halt.</p>\n<p>Users should note that the Dask dashboard can produce 500: Internal Server Error messages and go \"dead\" (all buttons turn grey) and  for minutes at a time, before \"coming back alive\" (all buttons turn orange).   It is possible that the computation is still active while these errors are occurring.</p>",
        "id": 17200,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599769903
    },
    {
        "content": "<p>I believe I understand better now why the Dask dashboard is not working for me on Cheyenne, while it is working for me on Casper.   </p>\n<p>On Casper, I was ssh-tunneling to my own installed version of Jupyter Lab, which is more recent.   Whereas on Cheyenne, I am logging in via JupyterHub, and using an older version of Jupyter.   Apparently the older version of Jupyter on JupyterHub does not have a properly configured Jupyter instance, or it is out of date, because the Dask dashboard there is constantly \"going dead\".</p>\n<p>If anyone knows how to run their own JupyterLab instance on Cheyenne, I would really appreciate some tips.   Thanks!</p>",
        "id": 17248,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599842498
    },
    {
        "content": "<p>you can use the same tunnelling approach on cheyenne...</p>",
        "id": 17249,
        "sender_full_name": "Deepak Cherian",
        "timestamp": 1599845334
    },
    {
        "content": "<p>Ah, thanks I didn't know that!</p>",
        "id": 17251,
        "sender_full_name": "Brian Bonnlander",
        "timestamp": 1599848816
    }
]