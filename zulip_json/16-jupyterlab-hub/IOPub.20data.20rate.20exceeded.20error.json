[
    {
        "content": "<p>I'm getting this error when trying to plot a figure. I'm running lines of plot that work with other data, and as far as I can tell just my xarray dataset I regridded with xesmf throws this. Searching the internet for solutions suggested that I do what the error message suggests and make a jupyter notebook config file to raise the data rate limit, but I'm not sure what that does, exactly, or if I could, so am hesitant to try. Here's the error message:</p>\n<div class=\"codehilite\"><pre><span></span><code>msgpack.exceptions.ExtraData: unpack(b) received extra data.\nIOPub data rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_data_rate_limit`.\n\nCurrent values:\nServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nServerApp.rate_limit_window=3.0 (secs)\n</code></pre></div>\n<p>and the single line of plotting that caused the problem:</p>\n<div class=\"codehilite\"><pre><span></span><code>dsgr.oxy.isel(z_t=0).plot()\n</code></pre></div>\n<p>edit: the error also pops up when I run dsgr.compute() so the regridded data array must be too big, will try smaller chunks in dask</p>",
        "id": 102361,
        "sender_full_name": "Emma Hoffman",
        "timestamp": 1721150900
    },
    {
        "content": "<p>Alright, well, it doesn't seem to matter the chunk size, I'm regridding a single selected variable that's already been meaned across time, so its pretty reduced with 60 depths going from lat/lon of 145, 360 to 384, 320. The dask and xesmf github issues I'm reading through seem like this shouldn't be a problem, and my attempts to generate and change a config file to increase the rate limit manually don't seem to work. Not sure what else to try.</p>",
        "id": 102529,
        "sender_full_name": "Emma Hoffman",
        "timestamp": 1721258210
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"405\">Emma Hoffman</span> <a href=\"#narrow/stream/16-jupyterlab-hub/topic/IOPub.20data.20rate.20exceeded.20error/near/102361\">said</a>:</p>\n<blockquote>\n<p>as far as I can tell just my xarray dataset I regridded with xesmf throws this</p>\n</blockquote>\n<p>combined with </p>\n<blockquote>\n<p>edit: the error also pops up when I run dsgr.compute() so the regridded data array must be too big, will try smaller chunks in dask</p>\n</blockquote>\n<p>Makes me wonder if you're running out of memory in the regrid step. Have you remapped other variables from this 145x360 grid to the 384x320 grid? Another possibility would be running out of memory in the time-mean step: are you reading in multiple time levels and computing the time mean in your notebook? <code>dask</code> will do that lazily, so it might be good to try to do the <code>.compute()</code> on the source grid data before remapping it (how many time levels are you reading in, and what dask resources are you requesting to parallelize the computation?)</p>",
        "id": 102540,
        "sender_full_name": "Michael Levy",
        "timestamp": 1721273161
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"10\">@Michael Levy</span> <br>\nI did run compute on the time mean and as a netcdf file, then read that back in to use in the regridding, so I know it's not coming from that step, at least. I just tried on temperature instead of my desired oxygen and the regridding worked fine, both compute() and plot() ran without errors and a plot spit out. Hmm.</p>\n<p>For dask, I was asking for 1 node, for 20 minutes, with 100GB and 30 workers. The dashboard never even started to load though, it didn't overload and kill workers, or anything, as far as I can tell.</p>",
        "id": 102541,
        "sender_full_name": "Emma Hoffman",
        "timestamp": 1721274550
    },
    {
        "content": "<p>Just tried oxygen again, and it seems to work? Plots are loading, at least, but when I tried subtracting the regridded data from the data originally on that grid, which is the original goal, and now I'm getting the IOPub data rate error when I run compute or plot on that. No idea what's going on.</p>",
        "id": 102542,
        "sender_full_name": "Emma Hoffman",
        "timestamp": 1721275439
    },
    {
        "content": "<p>Weird. Can you dump the regridded oxygen to netCDF and then read it back in? It definitely seems like a memory / resource limitation, but given your description there isn't really a resource-limited step. If you point me to your notebook, I'm happy to take a look and try to run it myself</p>",
        "id": 102545,
        "sender_full_name": "Michael Levy",
        "timestamp": 1721313402
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"10\">@Michael Levy</span> <br>\nThank you so much for your help. This is the path to the notebook I'm running:<br>\nu/home/emmah/1_Variability_Ventilation_O2/exploration_setup.ipynb</p>\n<p>when I tried to save the regridded file it failed to serialize with a \"0-dim memory has no length\" type error.</p>",
        "id": 102557,
        "sender_full_name": "Emma Hoffman",
        "timestamp": 1721317336
    },
    {
        "content": "<p>I started to poke around this afternoon, but couldn't find a conda environment to run it in (I tried cloning Yassir's <code>TPAC_CO2</code> environment but that's missing <code>geopy</code>). What environment are you using?</p>",
        "id": 102628,
        "sender_full_name": "Michael Levy",
        "timestamp": 1721341777
    },
    {
        "content": "<p>so sorry about that, I just updated the environment folder in the 1_Variability_Ventilation_O2 folder</p>",
        "id": 102629,
        "sender_full_name": "Emma Hoffman",
        "timestamp": 1721342786
    },
    {
        "content": "<p>I was able to install your environment and play with your notebook some. I don't have any definitive answers, but I do have a few suggestions:</p>\n<ol>\n<li>10 minutes is pretty short for keeping your workers around - I would request 30 minutes (<code>C=CLSTR(1,\"00:30\",100,30)</code>)</li>\n<li>The file <code>/glade/derecho/scratch/yeddebba/FOSI/LR/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch.pop.h.O2.195801-202112.nc</code> is a netCDF4 file, and it is already written in chunks - each time level is its own chunk:</li>\n</ol>\n<div class=\"codehilite\"><pre><span></span><code>$ ncdump -hs /glade/derecho/scratch/yeddebba/FOSI/LR/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch.pop.h.O2.195801-202112.nc | grep O2\nnetcdf g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch.pop.h.O2.195801-202112 {\n    float O2(time, z_t, nlat, nlon) ;\n        O2:long_name = &quot;Dissolved Oxygen&quot; ;\n        O2:units = &quot;mmol/m^3&quot; ;\n        O2:coordinates = &quot;TLONG TLAT z_t &quot; ;\n        O2:grid_loc = &quot;3111&quot; ;\n        O2:cell_methods = &quot;time: mean&quot; ;\n        O2:_FillValue = 9.96921e+36f ;\n        O2:missing_value = 9.96921e+36f ;\n        O2:_Storage = &quot;chunked&quot; ;\n        O2:_ChunkSizes = 1, 60, 384, 320 ;\n        O2:_Shuffle = &quot;true&quot; ;\n        O2:_DeflateLevel = 1 ;\n        O2:_Endianness = &quot;little&quot; ;\n        :history = &quot;Wed Jun 21 08:40:54 2023: ncks -O -4 -L 1 /glade/scratch/kristenk/archive/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch/ocn/proc/tseries/month_1/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch.pop.h.O2.195801-202112.nc /glade/scratch/kristenk/archive/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch/ocn/proc/tseries/month_1/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.4p2z.002branch.pop.h.O2.195801-202112.nc\\nnone&quot; ;\n</code></pre></div>\n<p>I think that changing from <code>chunks={'nlon':60,'nlat':60,'z_t':30}</code> to <code>chunks={'time':4}</code> will avoid some re-chunking. I know the goal is to average over all the time values, so it seems beneficial to have each chunk contain all the time levels, but in my experience rechunking tends to be expensive and should be done sparingly.</p>\n<p>Also, it might be worthwhile to create your own copy of <code>/glade/work/yeddebba/GOBAI_O2/GOBAI-O2-v2.1.nc</code> with the longitudes sorted correctly -- that would be more efficient than using <code>sortby</code> every time you read the file . Alternatively (or additionally), you could also read each file once and then do subsetting from memory -- e.g. your initial plotting cell could change to</p>\n<div class=\"codehilite\" data-code-language=\"Diff\"><pre><span></span><code>dircsl='/glade/derecho/scratch/yeddebba/FOSI/LR/'\ndsl=xr.open_mfdataset(dircsl+'*.O2.*.nc')\n<span class=\"gd\">-dsl_150= xr.open_mfdataset(dircsl+'*.O2.*.nc',chunks={'z_t':20,'time':100}).sel(z_t=15000,method='nearest')</span>\n<span class=\"gd\">-dsl_300= xr.open_mfdataset(dircsl+'*.O2.*.nc',chunks={'z_t':20,'time':100}).sel(z_t=30000,method='nearest')</span>\n<span class=\"gd\">-dsl_500= xr.open_mfdataset(dircsl+'*.O2.*.nc',chunks={'z_t':20,'time':100}).sel(z_t=50000,method='nearest')</span>\n<span class=\"gi\">+dsl_150=dsl.sel(z_t=15000,method='nearest')</span>\n<span class=\"gi\">+dsl_300=dsl.sel(z_t=30000,method='nearest')</span>\n<span class=\"gi\">+dsl_500=dsl.sel(z_t=50000,method='nearest')</span>\n</code></pre></div>\n<p>Or maybe <code>dsl</code> is already in memory from earlier in the notebook. If you want to chat about it, I'm happy to hop on a zoom call or google meet this afternoon (sometime between 2p and 4p Mountain Time?) or next week.</p>",
        "id": 102636,
        "sender_full_name": "Michael Levy",
        "timestamp": 1721403172
    }
]